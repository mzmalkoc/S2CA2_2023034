{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070968b",
   "metadata": {},
   "source": [
    "# ===================================\n",
    "# MONGODB CONNECTION AND YCSB TEST\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39694515",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mongo --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa9fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"MongoDB\").getOrCreate()\n",
    "\n",
    "# Read CSV file with Spark DataFrame\n",
    "mongodb_df = spark.read.csv(\"/ProjectTweets.csv\", header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITILAZE THE MONGODB IN TERMINAL\n",
    "# !mongod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29c31c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Configure MongoDB Database Connection\n",
    "mongodb_df.write.format(\"mongodb\") \\\n",
    "                .option(\"uri\",\"mongodb://127.0.0.1:27017/\") \\\n",
    "                .option(\"database\",\"TweetDatabaseMongoDB\") \\\n",
    "                .option(\"collection\",\"TweetCollectionMongoDB\") \\\n",
    "                .mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame via MongoDB\n",
    "mongodb_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dd16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# YCSB TEST FOR MONGODB\n",
    "!/home/hduser/ycsb-0.17.0/bin/ycsb.sh run mongodb -P /home/hduser/ycsb-0.17.0/workloads/workloada -p mongodb.url=mongodb://localhost:27017 -p mongodb.database=TweetDatabaseMongoDB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e187f1",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# MYSQL CONNECTION AND YCSB TEST\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mysql --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef47e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"MySQL\").getOrCreate()\n",
    "\n",
    "# Read CSV file with Spark DataFrame\n",
    "mysql_df = spark.read.csv(\"/ProjectTweets.csv\", header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fb3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITILAZE THE MYSQL IN TERMINAL\n",
    "# !mysql -u root -p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Configure MySQL Database Connection\n",
    "MySQL_Configuration = {\n",
    "    \"url\": \"jdbc:mysql://localhost:3306/sample\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"dbtable\": \"yourtable\",\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\"\n",
    "}\n",
    "\n",
    "# Load DataFrame into MySQL database\n",
    "mysql_df.write.format(\"jdbc\").options(**MySQL_Configuration).mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d06df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read DataFrame via MySQL\n",
    "mysql_df = spark.read.jdbc(url = \"jdbc:mysql://localhost:3306/sample\",\n",
    "                           table = \"yourtable\",\n",
    "                           properties = MySQL_Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c26071",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Show DataFrame via MySQL\n",
    "mysql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2d106",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# YCSB TEST FOR MYSQL\n",
    "!/home/hduser/ycsb-0.17.0/bin/ycsb.sh run jdbc -P /home/hduser/ycsb-0.17.0/workloads/workloada -p db.url=jdbc:mysql://localhost:3306/sample -p db.user=root -p db.passwd=password -p db.driver=com.mysql.cj.jdbc.Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09369a9a",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# READ TO CSV FROM HDFS VIA SPARK\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"HDFSToCSV\").getOrCreate()\n",
    "\n",
    "# Read CSV file with Spark DataFrame\n",
    "df = spark.read.csv(\"/ProjectTweets.csv\", header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f876af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame First 5 Rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c856f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first method for renamed the column names\n",
    "# df = df.withColumnRenamed(\"_c0\", \"id\").withColumnRenamed(\"_c1\", \"stamp\").withColumnRenamed(\"_c2\", \"date\").withColumnRenamed(\"_c3\", \"flag\").withColumnRenamed(\"_c4\", \"user\").withColumnRenamed(\"_c5\", \"text\")\n",
    "\n",
    "# The second method for renamed the column names\n",
    "df = df.selectExpr(\"_c0 as ID\", \"_c1 as STAMP\", \"_c2 as DATE\", \"_c3 as FLAG\", \"_c4 as USER\", \"_c5 as TEXT\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ae87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"STAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count unique values\n",
    "for column in Columns:\n",
    "    unique_values = df.select(column).distinct()\n",
    "    unique_count = unique_values.count()\n",
    "    \n",
    "    if unique_count > 0:\n",
    "        print(f\"{column} has {unique_count} unique values:\")\n",
    "    else:\n",
    "        print(f\"{column} has no unique value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"STAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count duplicate values\n",
    "for column in Columns:\n",
    "    count_df = df.groupBy(column).count()\n",
    "    duplicate_values = count_df.filter(col(\"count\") > 1).count()\n",
    "    \n",
    "    if duplicate_values > 0:\n",
    "        print(f\"{column} has {duplicate_values} duplicate values.\")\n",
    "    else:\n",
    "        print(f\"{column} has no duplicate value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ebf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the selected columns\n",
    "df = df.drop(\"ID, \"\"STAMP\", \"FLAG\", \"USER\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a848ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Do a grouping and counting operation to find duplicate values in the \"TEXT\" column\n",
    "count_df = df.groupBy(\"TEXT\").count()\n",
    "\n",
    "# Filter rows containing duplicate values\n",
    "duplicate_values = count_df.filter(col(\"count\") > 1)\n",
    "\n",
    "# If there are duplicate values, show them\n",
    "if duplicate_values.count() > 0:\n",
    "    print(\"Duplicate values:\")\n",
    "    duplicate_values.show(truncate=False)  # Display column values in full length\n",
    "else:\n",
    "    print(\"No duplicate values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f12ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988dd02",
   "metadata": {},
   "source": [
    "# =====================\n",
    "# TEXT PRE-PROCESSING\n",
    "# ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, lower, regexp_replace\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import torch\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer \n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"TEXT\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8ac18",
   "metadata": {},
   "source": [
    "#### TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebafbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s\\s+', ' ', text)\n",
    "    text = re.sub(r'[@_!#$%^&*()<>?/\\|}{~:]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Save as UDF\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Create new column\n",
    "df = df.withColumn(\"TEXT_C1\", clean_text_udf(col(\"text\")))\n",
    "df.select(\"TEXT\", \"TEXT_C1\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca560f3",
   "metadata": {},
   "source": [
    "#### EXPAND CONTRACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ba5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "# Save as UDF\n",
    "expand_contractions_udf = udf(expand_contractions, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "df = df.withColumn(\"TEXT_C2\", expand_contractions_udf(col(\"TEXT_C1\")))\n",
    "\n",
    "# Show the dataframe\n",
    "df.select(\"TEXT\", \"TEXT_C1\", \"TEXT_C2\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb546a",
   "metadata": {},
   "source": [
    "#### CLEAN THE PUNCTUATION CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b970ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define punctuation characters\n",
    "punctuation_characters = r'[!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-./:;<=>\\?@[\\\\]\\^_`{|}~]'\n",
    "\n",
    "# Remove punctuation characters\n",
    "df = df.withColumn(\"TEXT_C3\", regexp_replace(col(\"TEXT_C2\"), punctuation_characters, \"\"))\n",
    "\n",
    "# Show the dataframe\n",
    "df.select(\"TEXT\", \"TEXT_C1\", \"TEXT_C2\", \"TEXT_C3\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3af04d",
   "metadata": {},
   "source": [
    "#### CLEAN THE STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download \"stopwords\" from nltk dictionary\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Configure the language as english\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define the udf \n",
    "remove_stopwords_udf = udf(lambda text: \" \".join([word for word in text.split() if word not in stop_words]), StringType())\n",
    "\n",
    "# Use the UDF in order to remove stopwords and Create new column\n",
    "df = df.withColumn(\"TEXT_C4\", remove_stopwords_udf(col(\"TEXT_C3\")))\n",
    "\n",
    "# Show the dataframe\n",
    "df.select(\"TEXT\", \"TEXT_C1\", \"TEXT_C2\", \"TEXT_C3\", \"TEXT_C4\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad341b",
   "metadata": {},
   "source": [
    "#### IMPLEMENT LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download 'punkt','averaged_perceptron_tagger','wordnet' from nltk dictionary\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Defining the function that implements the Lemmatization operation as a UDF\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w)\n",
    "        lemmatized_sentence.append(lemma)\n",
    "    lemmatized_text = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Define the UDF\n",
    "lemmatize_text_udf = udf(lemmatize_text, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "df = df.withColumn(\"TEXT_C5\", lemmatize_text_udf(df[\"TEXT_C4\"]))\n",
    "\n",
    "# Show the dataframe\n",
    "df.select(\"TEXT\", \"TEXT_C1\", \"TEXT_C2\", \"TEXT_C3\", \"TEXT_C4\", \"TEXT_C5\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932161f9",
   "metadata": {},
   "source": [
    "#### IMPLEMENT STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c676df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Defining the function that finds word roots as UDF \n",
    "def stem_text(text):\n",
    "    snow = SnowballStemmer('english')\n",
    "    stemmed_sentence = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        stemmed_sentence.append(snow.stem(w))\n",
    "    stemmed_text = \" \".join(stemmed_sentence)\n",
    "    return stemmed_text\n",
    "\n",
    "# Define the UDF\n",
    "stem_text_udf = udf(stem_text, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "df = df.withColumn(\"TEXT_C6\", stem_text_udf(df[\"TEXT_C5\"]))\n",
    "\n",
    "# Show the dataframe\n",
    "df.select(\"TEXT\", \"TEXT_C1\", \"TEXT_C2\", \"TEXT_C3\", \"TEXT_C4\", \"TEXT_C5\", \"TEXT_C6\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e4dc6",
   "metadata": {},
   "source": [
    "#### IMPLEMENT TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function that splits text into tokens using NLTK\n",
    "def tokenize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# Define the UDF\n",
    "tokenize_text_udf = udf(tokenize_text, ArrayType(StringType()))\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "df = df.withColumn(\"TOKENS\", tokenize_text_udf(df[\"TEXT_C6\"]))\n",
    "\n",
    "# Show the selected dataframe\n",
    "df.select(\"TEXT\", \"TEXT_C6\", \"TOKENS\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a12f5",
   "metadata": {},
   "source": [
    "#### DROP THE REDUNDANT COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88222d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce561c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the selected columns\n",
    "df = df.drop(\"TEXT_C1\", \"TEXT_C2\", \"TEXT_C3\", \"TEXT_C4\", \"TEXT_C5\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e720fc7",
   "metadata": {},
   "source": [
    "#### IMPLEMENT TOKENIZATION AND SPLIT WORDS TO ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Create The Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"TEXT_C6\", outputCol=\"words\")\n",
    "tokenizer_df = tokenizer.transform(df)\n",
    "\n",
    "# Separate words into individual lines\n",
    "tokenizer_df = tokenizer_df.select(explode(col(\"words\")).alias(\"word\"))\n",
    "\n",
    "# Show the dataframe\n",
    "tokenizer_df.show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f10c9",
   "metadata": {},
   "source": [
    "#### COUNT THE TOKENIZER WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Count the tokenizer words\n",
    "tokenizer_df_count = tokenizer_df.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Show the dataframe\n",
    "tokenizer_df_count.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf963ad",
   "metadata": {},
   "source": [
    "## ==============\n",
    "## Text Blob Method\n",
    "## ==============\n",
    "#### SENTIMENT SCORES AND SENTIMENT LABEL ( POSITIVE - NEGATIVE - NEUTRAL )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c03903",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sentimental Analysis Function\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# Sentiment Label Function\n",
    "def label_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'positive'\n",
    "    elif score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "#=========================================================================================#\n",
    "\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "\n",
    "# Define the UDF with Functions\n",
    "sentiment_udf = udf(get_sentiment, FloatType())\n",
    "label_udf = udf(label_sentiment, StringType())\n",
    "\n",
    "# Use the UDF and Create new columns\n",
    "df = df.withColumn('SentimentScores', sentiment_udf(df['TEXT_C6']))\n",
    "df = df.withColumn('SentimentLabels', label_udf(df['SentimentScores']))\n",
    "\n",
    "# Count and Show the 'sentiment_label' column\n",
    "df.groupBy('SentimentLabels').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd1cdb2",
   "metadata": {},
   "source": [
    "## ==================\n",
    "## Vader Lexicon Method\n",
    "## ==================\n",
    "#### SENTIMENT SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function for sentiment analysis that filters sentiment words\n",
    "def filter_sentiment_words(text):\n",
    "    # Create SentimentIntensityAnalyzer object\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "    # Select words that express sentiment\n",
    "    filtered_words = [word for word in words if sia.polarity_scores(word)['compound'] != 0]\n",
    "    return filtered_words\n",
    "\n",
    "# Define a UDF for Spark\n",
    "filter_sentiment_udf = udf(filter_sentiment_words, ArrayType(StringType()))\n",
    "\n",
    "# Define a UDF to get the sentiment score\n",
    "sentiment_score_udf = udf(lambda text: SentimentIntensityAnalyzer().polarity_scores(text)['compound'], FloatType())\n",
    "\n",
    "# Apply the UDFs to the 'text' column\n",
    "df = df.withColumn(\"SentimentWords\", filter_sentiment_udf(col(\"TEXT_C6\")))\n",
    "df = df.withColumn(\"NewSentimentScores\", sentiment_score_udf(col(\"TEXT_C6\")))\n",
    "\n",
    "# Select the necessary columns and rename the 'sentiment_words' column to 'text'\n",
    "df.select(\"DATE\", \"SentimentWords\", \"NewSentimentScores\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc21e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Sentiment Label Function\n",
    "def new_label_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'positive'\n",
    "    elif score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Define the UDF with Function\n",
    "new_label_udf = udf(new_label_sentiment, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "df = df.withColumn('NewSentimentLabels', new_label_udf(df['NewSentimentScores']))\n",
    "\n",
    "# Count and Show the 'NewSentimentLabels' column\n",
    "df.groupBy('NewSentimentLabels').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c0963",
   "metadata": {},
   "source": [
    "## Sentiment Score Comparison of TextBlob and Vader Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"TEXT\", \"SentimentScores\", \"NewSentimentScores\").show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4c559",
   "metadata": {},
   "source": [
    "#### DROP THE REDUNDANT COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f2808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4266dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the selected columns\n",
    "df = df.drop(\"TEXT_C1\", \"TEXT_C2\", \"TEXT_C3\", \"TEXT_C4\", \"TEXT_C5\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de13332",
   "metadata": {},
   "source": [
    "#### POSITIVE AND NEGATIVE WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.NewSentimentLabels == 'positive'].TEXT_C6))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f633d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20)) \n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.NewSentimentLabels == 'negative'].TEXT_C6))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617eb45f",
   "metadata": {},
   "source": [
    "#### The Positive, Neutral and Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3de32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Count the whole text \n",
    "total_count = df.count()\n",
    "# Count the positive sentiment label\n",
    "positive_count = df.filter(df.NewSentimentLabels == \"positive\").count()\n",
    "# Calculate the positive rate\n",
    "positive_rate = (positive_count / total_count) * 100\n",
    "# Print the positive rate\n",
    "print(f\"Positive count: {positive_count}\")\n",
    "print(f\"Positive rate : {positive_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Count the whole text \n",
    "total_count = df.count()\n",
    "# Count the neutral sentiment label\n",
    "neutral_count = df.filter(df.NewSentimentLabels == \"neutral\").count()\n",
    "# Calculate the neutral rate\n",
    "neutral_rate = (neutral_count / total_count) * 100\n",
    "# Print the neutral rate\n",
    "print(f\"Neutral count: {neutral_count}\")\n",
    "print(f\"Neutral rate : {neutral_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Count the whole text \n",
    "total_count = df.count()\n",
    "# Count the negative sentiment label\n",
    "negative_count = df.filter(df.NewSentimentLabels == \"negative\").count()\n",
    "# Calculate the negative rate\n",
    "negative_rate = (negative_count / total_count) * 100\n",
    "# Print the negative rate\n",
    "print(f\"Negative count: {negative_count}\")\n",
    "print(f\"Negative rate : {negative_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6fff9",
   "metadata": {},
   "source": [
    "# ===============================================\n",
    "# TIMESTAMP PREPARATION FOR TIME SERIES ANALYSIS\n",
    "# ================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6f3eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show the schema of the dataframe\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78250158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the Selected Columns\n",
    "df.select(\"DATE\", \"TEXT\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c26b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Dataframe\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Convert to 'yyyy-MM-dd HH:mm:ss' format\n",
    "df = df.withColumn(\"TIMESTAMP\", to_timestamp(col(\"DATE\"), \"EEE MMM dd HH:mm:ss zzz yyyy\"))\n",
    "\n",
    "# Show the selected columns\n",
    "df.select(\"DATE\", \"TIMESTAMP\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251c1da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COnvert to 'yyyy-MM-dd' format\n",
    "df = df.withColumn(\"YearMonthDate\", col(\"TIMESTAMP\").substr(1, 10))\n",
    "\n",
    "# Show the selected columns\n",
    "df.select(\"TIMESTAMP\", \"YearMonthDate\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort ascending via TIMESTAMP column\n",
    "df = df.orderBy(\"TIMESTAMP\", ascending=True)\n",
    "\n",
    "# Show sorted DataFrame\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75c8ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# YearMonthDate sütununu DateType'a çevirme\n",
    "date_df = date_df.withColumn(\"YearMonthDateTYPE\", col(\"YearMonthDate\").cast(DateType()))\n",
    "\n",
    "# \"THE OLDEST DATE\" ve \"THE NEWEST DATE\" bulma\n",
    "oldest_date = date_df.agg({\"YearMonthDateTYPE\": \"min\"}).collect()[0][0]\n",
    "newest_date = date_df.agg({\"YearMonthDateTYPE\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(\"THE OLDEST DATE:\", oldest_date)\n",
    "print(\"THE NEWEST DATE:\", newest_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e318356",
   "metadata": {},
   "source": [
    "# B U N A     -     B I      -    B A K\n",
    "### EKSIK GUNLERI NASIL BULACAGIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5279eec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f2ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6273cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb2c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1684b887",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# MYSQL CONNECTION AND YCSB TEST\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbcd83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf70ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YCSB TEST FOR MYSQL\n",
    "!/home/hduser/ycsb-0.17.0/bin/ycsb.sh run jdbc -P /home/hduser/ycsb-0.17.0/workloads/workloada -p db.url=jdbc:mysql://localhost:3306/tweet_mysql -p db.user=root -p db.passwd=kalem -p db.driver=com.mysql.cj.jdbc.Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3bdccc",
   "metadata": {},
   "source": [
    "# ===================================\n",
    "# MONGODB CONNECTION AND YCSB TEST\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e0050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee047410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YCSB TEST FOR MONGODB\n",
    "!/home/hduser/ycsb-0.17.0/bin/ycsb.sh run mongodb -P /home/hduser/ycsb-0.17.0/workloads/workloada -p mongodb.url=mongodb://localhost:27017 -p mongodb.database=tweet_mongo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb92fa5d",
   "metadata": {},
   "source": [
    "# ==========================\n",
    "# DEEP LEARNING - RNN MODEL\n",
    "# =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdaac11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "208632bd",
   "metadata": {},
   "source": [
    "# ======\n",
    "## GRAPHS\n",
    "# ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02774035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# 1-Week Analysis\n",
    "weekly_data = date_df.groupBy(date_format(\"TIMESTAMP\", \"yyyy-ww\")).count()\n",
    "weekly_data = weekly_data.withColumnRenamed(\"date_format(TIMESTAMP, yyyy-ww)\", \"week\")\n",
    "weekly_data = weekly_data.orderBy(\"week\", ascending=True)\n",
    "weekly_data.show()\n",
    "\n",
    "# Get the result and visualize it\n",
    "weekly_data_pd = weekly_data.toPandas()\n",
    "\n",
    "# Plot for 1-Week Time Series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(weekly_data_pd[\"week\"], weekly_data_pd[\"count\"], width=0.5)\n",
    "plt.title(\"Weekly Tweet Count\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# 1-Month Analysis\n",
    "monthly_data = date_df.groupBy(date_format(\"TIMESTAMP\", \"yyyy-MM\")).count()\n",
    "monthly_data = monthly_data.withColumnRenamed(\"date_format(TIMESTAMP, yyyy-MM)\", \"month\")\n",
    "monthly_data.show()\n",
    "\n",
    "# Get the results and visualize it\n",
    "monthly_data_pd = monthly_data.toPandas()\n",
    "\n",
    "# Plot for 1-Month Time Series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(monthly_data_pd[\"month\"], monthly_data_pd[\"count\"], width=0.5)\n",
    "plt.title(\"Monthly Tweet Count\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# 3-Month Analysis\n",
    "quarterly_data = date_df.groupBy(date_format(\"TIMESTAMP\", \"yyyy-MM\")).count()\n",
    "quarterly_data = quarterly_data.withColumnRenamed(\"date_format(TIMESTAMP, yyyy-MM)\", \"quarter\")\n",
    "quarterly_data.show()\n",
    "\n",
    "# Get the results and visualize it\n",
    "quarterly_data_pd = quarterly_data.toPandas()\n",
    "\n",
    "# Plot for 3-Month Time Series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(quarterly_data_pd[\"quarter\"], quarterly_data_pd[\"count\"], width=0.5)\n",
    "plt.title(\"3-Month Tweet Count\")\n",
    "plt.xlabel(\"Quarter\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e51f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
