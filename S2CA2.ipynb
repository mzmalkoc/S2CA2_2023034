{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070968b",
   "metadata": {},
   "source": [
    "### CONNECTING TO MONGODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39694515",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mongo --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mongosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MongoDB\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DFToMongoDB\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# # #\n",
    "data = [(\"John\", 28), (\"Alice\", 22), (\"Bob\", 32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c80aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f56ac60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure MongoDB Database Connection\n",
    "df.write.format(\"mongodb\") \\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1:27017/\") \\\n",
    "    .option(\"database\",\"sample_db\") \\\n",
    "    .option(\"collection\",\"scb\") \\\n",
    "    .mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e187f1",
   "metadata": {},
   "source": [
    "### CONNECTING TO MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mysql --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MySQL\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DFToMySQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# # #\n",
    "data = [(\"John\", 28), (\"Alice\", 22), (\"Bob\", 32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Configure MySQL Database Connection\n",
    "mysql_options = {\n",
    "    \"url\": \"jdbc:mysql://localhost:3306/sample\",  # MySQL bağlantı URL'si\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",  # MySQL JDBC sürücüsü\n",
    "    \"dbtable\": \"yourtable\",  # Hedef MySQL tablo adı\n",
    "    \"user\": \"root\",  # MySQL kullanıcı adı\n",
    "    \"password\": \"password\"  # MySQL parola\n",
    "}\n",
    "\n",
    "# DataFrame'i MySQL veritabanına yükleyin\n",
    "df.write.format(\"jdbc\").options(**mysql_options).mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25733419",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat zahid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09369a9a",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# READ TO CSV FROM HDFS VIA SPARK\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae976e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"HDFSToCSV\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Specify CSV file path throught HDFS\n",
    "hdfs_file_path = \"/ProjectTweets.csv\"\n",
    "\n",
    "# Read CSV file with Spark DataFrame\n",
    "df = spark.read.csv(hdfs_file_path, header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f876af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame First 5 Rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbe39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first method for renamed the column names\n",
    "df1 = df.withColumnRenamed(\"_c0\", \"id\").withColumnRenamed(\"_c1\", \"stamp\").withColumnRenamed(\"_c2\", \"date\").withColumnRenamed(\"_c3\", \"flag\").withColumnRenamed(\"_c4\", \"user\").withColumnRenamed(\"_c5\", \"text\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c856f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| ID|     STAMP|                DATE|    FLAG|           USER|                TEXT|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The second method for renamed the column names\n",
    "df = df.selectExpr(\"_c0 as ID\", \"_c1 as STAMP\", \"_c2 as DATE\", \"_c3 as FLAG\", \"_c4 as USER\", \"_c5 as TEXT\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ae87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"STAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count unique values\n",
    "for column in Columns:\n",
    "    unique_values = df.select(column).distinct()\n",
    "    unique_count = unique_values.count()\n",
    "    \n",
    "    if unique_count > 0:\n",
    "        print(f\"{column} has {unique_count} unique values:\")\n",
    "    else:\n",
    "        print(f\"{column} has no unique value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"STAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count duplicate values\n",
    "for column in Columns:\n",
    "    count_df = df.groupBy(column).count()\n",
    "    duplicate_values = count_df.filter(col(\"count\") > 1).count()\n",
    "    \n",
    "    if duplicate_values > 0:\n",
    "        print(f\"{column} has {duplicate_values} duplicate values.\")\n",
    "    else:\n",
    "        print(f\"{column} has no duplicate value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2ebf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the selected columns\n",
    "df = df.drop(\"STAMP\", \"FLAG\", \"USER\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a848ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Do a grouping and counting operation to find duplicate values in the \"TEXT\" column\n",
    "count_df = df.groupBy(\"TEXT\").count()\n",
    "\n",
    "# Filter rows containing duplicate values\n",
    "duplicate_values = count_df.filter(col(\"count\") > 1)\n",
    "\n",
    "# If there are duplicate values, show them\n",
    "if duplicate_values.count() > 0:\n",
    "    print(\"Duplicate values:\")\n",
    "    duplicate_values.show(truncate=False)  # Display column values in full length\n",
    "else:\n",
    "    print(\"No duplicate values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f12ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988dd02",
   "metadata": {},
   "source": [
    "# ====================\n",
    "# TEXT PRE-PROCESSING\n",
    "# ====================\n",
    "\n",
    "standard pre-processing techniques:\n",
    "\n",
    "- Lower casing the corpus \n",
    "- Removing the punctuation \n",
    "- Removing the stopwords \n",
    "- Tokenizing the corpus \n",
    "- Stemming and Lemmatization\n",
    "- Word embeddings using CountVectorizer and TF-IDF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd24a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = df.select(\"TEXT\")\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, lower, regexp_replace\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import torch\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8ac18",
   "metadata": {},
   "source": [
    "#### TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd2a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Save as UDF\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C1\", clean_text_udf(col(\"text\")))\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca560f3",
   "metadata": {},
   "source": [
    "#### EXPAND CONTRACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104ba5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "# Save as UDF\n",
    "expand_contractions_udf = udf(expand_contractions, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C2\", expand_contractions_udf(col(\"TEXT_C1\")))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb546a",
   "metadata": {},
   "source": [
    "#### CLEAN THE PUNCTUATION CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b970ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define punctuation characters\n",
    "punctuation_characters = r'[!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-./:;<=>\\?@[\\\\]\\^_`{|}~]'\n",
    "\n",
    "# Remove punctuation characters\n",
    "text_df = text_df.withColumn(\"TEXT_C3\", regexp_replace(col(\"TEXT_C2\"), punctuation_characters, \"\"))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3af04d",
   "metadata": {},
   "source": [
    "#### CLEAN THE STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download \"stopwords\" from nltk dictionary\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Configure the language as english\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define the udf \n",
    "remove_stopwords_udf = udf(lambda text: \" \".join([word for word in text.split() if word not in stop_words]), StringType())\n",
    "\n",
    "# Use the UDF in order to remove stopwords and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C4\", remove_stopwords_udf(col(\"TEXT_C3\")))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad341b",
   "metadata": {},
   "source": [
    "#### IMPLEMENT LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download 'punkt','averaged_perceptron_tagger','wordnet' from nltk dictionary\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Defining the function that implements the Lemmatization operation as a UDF\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w)\n",
    "        lemmatized_sentence.append(lemma)\n",
    "    lemmatized_text = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Define the UDF\n",
    "lemmatize_text_udf = udf(lemmatize_text, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C5\", lemmatize_text_udf(text_df[\"TEXT_C4\"]))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932161f9",
   "metadata": {},
   "source": [
    "#### IMPLEMENT STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c676df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Defining the function that finds word roots as UDF \n",
    "def stem_text(text):\n",
    "    snow = SnowballStemmer('english')\n",
    "    stemmed_sentence = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        stemmed_sentence.append(snow.stem(w))\n",
    "    stemmed_text = \" \".join(stemmed_sentence)\n",
    "    return stemmed_text\n",
    "\n",
    "# Define the UDF\n",
    "stem_text_udf = udf(stem_text, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C6\", stem_text_udf(text_df[\"TEXT_C5\"]))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e4dc6",
   "metadata": {},
   "source": [
    "#### IMPLEMENT TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function that splits text into tokens using NLTK\n",
    "def tokenize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# Define the UDF\n",
    "tokenize_text_udf = udf(tokenize_text, ArrayType(StringType()))\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"tokens\", tokenize_text_udf(text_df[\"TEXT_C6\"]))\n",
    "\n",
    "# Show the selected dataframe\n",
    "text_df.select(\"TEXT_C6\", \"tokens\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e720fc7",
   "metadata": {},
   "source": [
    "#### IMPLEMENT TOKENIZATION AND SPLIT WORDS TO ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Create The Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"TEXT_C6\", outputCol=\"words\")\n",
    "tokenizer_df = tokenizer.transform(text_df)\n",
    "\n",
    "# Separate words into individual lines\n",
    "tokenizer_df = tokenizer_df.select(explode(col(\"words\")).alias(\"word\"))\n",
    "\n",
    "# Show the dataframe\n",
    "tokenizer_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f10c9",
   "metadata": {},
   "source": [
    "#### COUNT THE TOKENIZER WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the tokenizer words\n",
    "tokenizer_df_count = tokenizer_df.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Show the dataframe\n",
    "tokenizer_df_count.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf963ad",
   "metadata": {},
   "source": [
    "#### SENTIMENT LABEL ( POSITIVE - NEGATIVE - NEUTRAL )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c03903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Sentimental Analysis Function\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# Sentiment Label Function\n",
    "def label_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'positive'\n",
    "    elif score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "===================================================\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "\n",
    "# Define the UDF with Functions\n",
    "sentiment_udf = udf(get_sentiment, FloatType())\n",
    "label_udf = udf(label_sentiment, StringType())\n",
    "\n",
    "# Use the UDF and Create new columns\n",
    "text_df = text_df.withColumn('sentiment_score', sentiment_udf(text_df['TEXT_C6']))\n",
    "text_df = text_df.withColumn('sentiment_label', label_udf(text_df['sentiment_score']))\n",
    "\n",
    "# Count and Show the 'sentiment_label' column\n",
    "text_df.groupBy('sentiment_label').count().show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c1c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the whole text \n",
    "total_count = text_df.count()\n",
    "\n",
    "# Count the positive, neutral and negative sentiment label\n",
    "positive_count = text_df.filter(text_df.sentiment_label == \"positive\").count()\n",
    "neutral_count = text_df.filter(text_df.sentiment_label == \"neutral\").count()\n",
    "negative_count = text_df.filter(text_df.sentiment_label == \"negative\").count()\n",
    "\n",
    "# Calculate the positive, neutral and negative rate\n",
    "positive_rate = (positive_count / total_count) * 100\n",
    "neutral_rate = (neutral_count / total_count) * 100\n",
    "negative_rate = (negative_count / total_count) * 100\n",
    "\n",
    "# Print the positive, neutral and negative rate\n",
    "print(f\"Positive rate: {positive_rate}%\")\n",
    "print(f\"Neutral rate: {neutral_rate}%\")\n",
    "print(f\"Negative rate: {negative_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ea7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Duygu etiketlerini yeni bir sütunda saklama\n",
    "text_df = text_df.withColumn(\"sentiment_label_column\", \n",
    "     when(text_df[\"sentiment_label\"] == \"positive\", \"positive\")\n",
    "    .when(text_df[\"sentiment_label\"] == \"neutral\", \"neutral\")\n",
    "    .when(text_df[\"sentiment_label\"] == \"negative\", \"negative\")\n",
    "    .otherwise(\"unknown\")\n",
    ")\n",
    "\n",
    "# Show the selected columns\n",
    "text_df.select(\"TEXT_C6\", \"sentiment_label_column\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb1ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures polarity\n",
    "def get_polarity(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity \n",
    "\n",
    "# Define UDF\n",
    "polarity_udf = udf(get_polarity, FloatType())\n",
    "\n",
    "# Calculate polarity score for each text in column 'TEXT_C6' and add to a new column\n",
    "text_df = text_df.withColumn(\"polarity_score\", polarity_udf(text_df['TEXT_C6']))\n",
    "\n",
    "# Show the new column\n",
    "text_df.select(\"TEXT_C6\", \"polarity_score\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6fff9",
   "metadata": {},
   "source": [
    "# ====================\n",
    "# TIME SERIES ANALYSIS\n",
    "# ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78250158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|DATE                        |\n",
      "+----------------------------+\n",
      "|Mon Apr 06 22:19:45 PDT 2009|\n",
      "|Mon Apr 06 22:19:49 PDT 2009|\n",
      "|Mon Apr 06 22:19:53 PDT 2009|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|\n",
      "|Mon Apr 06 22:20:00 PDT 2009|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|\n",
      "|Mon Apr 06 22:20:05 PDT 2009|\n",
      "|Mon Apr 06 22:20:09 PDT 2009|\n",
      "|Mon Apr 06 22:20:16 PDT 2009|\n",
      "|Mon Apr 06 22:20:17 PDT 2009|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|\n",
      "|Mon Apr 06 22:20:22 PDT 2009|\n",
      "|Mon Apr 06 22:20:25 PDT 2009|\n",
      "|Mon Apr 06 22:20:31 PDT 2009|\n",
      "|Mon Apr 06 22:20:34 PDT 2009|\n",
      "+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the DATE column\n",
    "date_df = df.select(\"DATE\")\n",
    "\n",
    "# Show the DATE column\n",
    "date_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6f3eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show the schema of the dataframe\n",
    "date_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ec73a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"Legacy\")\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "timestamp = to_timestamp(\"Mon Apr 06 22:19:45 PDT 2009\", \"EEE MMM dd HH:mm:ss Z yyyy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe8b8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------+\n",
      "|DATE                        |TIMESTAMP          |\n",
      "+----------------------------+-------------------+\n",
      "|Mon Apr 06 22:19:45 PDT 2009|2009-04-07 06:19:45|\n",
      "|Mon Apr 06 22:19:49 PDT 2009|2009-04-07 06:19:49|\n",
      "|Mon Apr 06 22:19:53 PDT 2009|2009-04-07 06:19:53|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|2009-04-07 06:19:57|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|2009-04-07 06:19:57|\n",
      "+----------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "date_df = date_df.withColumn(\"TIMESTAMP\", F.to_timestamp(date_df[\"DATE\"], \"EEE MMM dd HH:mm:ss Z yyyy\"))\n",
    "\n",
    "date_df = date_df.withColumn(\"TIMESTAMP\", F.col(\"TIMESTAMP\").cast(TimestampType()))\n",
    "\n",
    "date_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort ascending via TIMESTAMP column\n",
    "date_df = date_df.orderBy(\"TIMESTAMP\", ascending=True)\n",
    "\n",
    "# Show sorted DataFrame\n",
    "date_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f64e247b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE OLDEST DATE: 2009-04-07 06:19:45\n"
     ]
    }
   ],
   "source": [
    "# Select THE NEWEST DATE\n",
    "newest_date = date_df.select(\"TIMESTAMP\").first()[0]\n",
    "\n",
    "# Print THE NEWEST DATE\n",
    "print(\"THE NEWEST DATE:\", newest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6568a8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE OLDEST DATE: 2009-06-16 16:40:50\n"
     ]
    }
   ],
   "source": [
    "# Select THE OLDEST DATE\n",
    "oldest_date = date_df.select(\"TIMESTAMP\").collect()[-1][0]\n",
    "\n",
    "# Print THE OLDEST DATE\n",
    "print(\"THE OLDEST DATE:\", oldest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02774035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# 1-Week Analysis\n",
    "weekly_data = date_df.groupBy(date_format(\"TIMESTAMP\", \"yyyy-ww\")).count()\n",
    "weekly_data = weekly_data.withColumnRenamed(\"date_format(TIMESTAMP, yyyy-ww)\", \"week\")\n",
    "weekly_data = weekly_data.orderBy(\"week\", ascending=True)\n",
    "weekly_data.show()\n",
    "\n",
    "# Get the result and visualize it\n",
    "weekly_data_pd = weekly_data.toPandas()\n",
    "\n",
    "# Plot for 1-Week Time Series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(weekly_data_pd[\"week\"], weekly_data_pd[\"count\"], width=0.5)\n",
    "plt.title(\"Weekly Tweet Count\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# 1-Month Analysis\n",
    "monthly_data = date_df.groupBy(date_format(\"TIMESTAMP\", \"yyyy-MM\")).count()\n",
    "monthly_data = monthly_data.withColumnRenamed(\"date_format(TIMESTAMP, yyyy-MM)\", \"month\")\n",
    "monthly_data.show()\n",
    "\n",
    "# Get the results and visualize it\n",
    "monthly_data_pd = monthly_data.toPandas()\n",
    "\n",
    "# Plot for 1-Month Time Series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(monthly_data_pd[\"month\"], monthly_data_pd[\"count\"], width=0.5)\n",
    "plt.title(\"Monthly Tweet Count\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# 3-Month Analysis\n",
    "quarterly_data = date_df.groupBy(date_format(\"TIMESTAMP\", \"yyyy-MM\")).count()\n",
    "quarterly_data = quarterly_data.withColumnRenamed(\"date_format(TIMESTAMP, yyyy-MM)\", \"quarter\")\n",
    "quarterly_data.show()\n",
    "\n",
    "# Get the results and visualize it\n",
    "quarterly_data_pd = quarterly_data.toPandas()\n",
    "\n",
    "# Plot for 3-Month Time Series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(quarterly_data_pd[\"quarter\"], quarterly_data_pd[\"count\"], width=0.5)\n",
    "plt.title(\"3-Month Tweet Count\")\n",
    "plt.xlabel(\"Quarter\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8605efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boş değerlerin olduğu satırlar:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|DATE|TIMESTAMP|\n",
      "+----+---------+\n",
      "+----+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam boş değer sayısı: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Boş değerleri kontrol etmek için isNull() kullanın\n",
    "nan_value = date_df.filter(F.col(\"TIMESTAMP\").isNull())\n",
    "\n",
    "# Hangi satırlarda boş değerler olduğunu gösterin\n",
    "print(\"Boş değerlerin olduğu satırlar:\")\n",
    "nan_value.show()\n",
    "\n",
    "# Toplam boş değer sayısını alın\n",
    "nan_value_count = nan_value.count()\n",
    "print(\"Toplam boş değer sayısı:\", nan_value_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e51f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
