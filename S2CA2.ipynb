{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf5b36e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070968b",
   "metadata": {},
   "source": [
    "### CONNECTING TO MONGODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39694515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB shell version: 3.2.10\r\n"
     ]
    }
   ],
   "source": [
    "!mongo --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mongosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc29c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MongoDB\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DFToMongoDB\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# # #\n",
    "data = [(\"John\", 28), (\"Alice\", 22), (\"Bob\", 32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c80aada",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| John| 28|\n",
      "|Alice| 22|\n",
      "|  Bob| 32|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:===========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f56ac60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Configure MongoDB Database Connection\n",
    "df.write.format(\"mongodb\") \\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1:27017/\") \\\n",
    "    .option(\"database\",\"sample_db\") \\\n",
    "    .option(\"collection\",\"scb\") \\\n",
    "    .mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e187f1",
   "metadata": {},
   "source": [
    "### CONNECTING TO MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb34813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql  Ver 8.0.30-0ubuntu0.22.04.1 for Linux on x86_64 ((Ubuntu))\r\n"
     ]
    }
   ],
   "source": [
    "!mysql --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edb6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MySQL\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DFToMySQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# # #\n",
    "data = [(\"John\", 28), (\"Alice\", 22), (\"Bob\", 32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Configure MySQL Database Connection\n",
    "mysql_options = {\n",
    "    \"url\": \"jdbc:mysql://localhost:3306/sample\",  # MySQL bağlantı URL'si\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",  # MySQL JDBC sürücüsü\n",
    "    \"dbtable\": \"yourtable\",  # Hedef MySQL tablo adı\n",
    "    \"user\": \"root\",  # MySQL kullanıcı adı\n",
    "    \"password\": \"password\"  # MySQL parola\n",
    "}\n",
    "\n",
    "# DataFrame'i MySQL veritabanına yükleyin\n",
    "df.write.format(\"jdbc\").options(**mysql_options).mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25733419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hduser/Desktop\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3879a6cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwd\r\n",
      "cd Downloads/\r\n",
      "nano zahid.txt\r\n",
      "cat zahid.txt \r\n",
      "-----------------------------------------------\r\n",
      "mysql -u root -p\r\n",
      "Enter password: password\r\n",
      "CREATE DATABASE sample;\r\n",
      "USE sample;\r\n",
      "SHOW TABLES;\r\n",
      "-----------------------------------------------\r\n",
      "Downloads$ ls mon*\r\n",
      "Downloads$ sudo cp mon* /usr/local/spark/jars\r\n",
      "Downloads$ sudo cp ./bson-3.12.12.jar /usr/local/spark/jars\r\n",
      "Downloads$ ls mys*\r\n",
      "Downloads$ sudo cp ./mysql-connector-j-8.0.33.jar /usr/local/spark/jars\r\n",
      "--------------------------------------------------------------------------\r\n",
      "cd /usr/local/spark/jars\r\n",
      "/usr/local/spark/jars$ ls mon*\r\n",
      "/usr/local/spark/jars$ ls mysql*\r\n",
      "--------------------------------------------------------------------------\r\n",
      "mongosh\r\n",
      "/mongodb-linux-x86_64-ubuntu1604-3.2.10$ ./bin/mongod\r\n",
      "--------------------------------------------------------------------------\r\n",
      "https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/3.12.12/\r\n",
      "--------------------------------------------------------------------------\r\n",
      "mysql --version\r\n",
      "mongo --version\r\n"
     ]
    }
   ],
   "source": [
    "!cat zahid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09369a9a",
   "metadata": {},
   "source": [
    "### READ TO CSV FROM HDFS VIA SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae976e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"HDFSToCSV\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Specify CSV file path throught HDFS\n",
    "hdfs_file_path = \"/ProjectTweets.csv\"\n",
    "\n",
    "# Read CSV file with Spark DataFrame\n",
    "df = spark.read.csv(hdfs_file_path, header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f876af7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show DataFrame First 5 Rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74e9815",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdbe39f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| id| timestamp|                date|    flag|           user|                text|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The first method for renamed the column names\n",
    "df1 = df.withColumnRenamed(\"_c0\", \"id\").withColumnRenamed(\"_c1\", \"timestamp\").withColumnRenamed(\"_c2\", \"date\").withColumnRenamed(\"_c3\", \"flag\").withColumnRenamed(\"_c4\", \"user\").withColumnRenamed(\"_c5\", \"text\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c856f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| ID| TIMESTAMP|                DATE|    FLAG|           USER|                TEXT|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The second method for renamed the column names\n",
    "df = df.selectExpr(\"_c0 as ID\", \"_c1 as TIMESTAMP\", \"_c2 as DATE\", \"_c3 as FLAG\", \"_c4 as USER\", \"_c5 as TEXT\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55ae87a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has 1600000 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ae8b46",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID has 1600000 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP has 1598315 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE has 774363 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG has 1 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER has 659775 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT has 1581466 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"TIMESTAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count unique values\n",
    "for column in Columns:\n",
    "    unique_values = df.select(column).distinct()\n",
    "    unique_count = unique_values.count()\n",
    "    \n",
    "    if unique_count > 0:\n",
    "        print(f\"{column} has {unique_count} unique values:\")\n",
    "    else:\n",
    "        print(f\"{column} has no unique value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8bbe08a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID has no duplicate value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP has 1685 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE has 373151 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG has 1 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER has 254498 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:==============================================>       (173 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT has 8434 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:===================================================>  (189 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"TIMESTAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count duplicate values\n",
    "for column in Columns:\n",
    "    count_df = df.groupBy(column).count()\n",
    "    duplicate_values = count_df.filter(col(\"count\") > 1).count()\n",
    "    \n",
    "    if duplicate_values > 0:\n",
    "        print(f\"{column} has {duplicate_values} duplicate values.\")\n",
    "    else:\n",
    "        print(f\"{column} has no duplicate value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2ebf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+--------------------+\n",
      "| ID|                DATE|           USER|                TEXT|\n",
      "+---+--------------------+---------------+--------------------+\n",
      "|  0|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|\n",
      "|  2|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|\n",
      "|  3|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|\n",
      "|  4|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|\n",
      "+---+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the selected columns\n",
    "df = df.drop(\"TIMESTAMP\", \"FLAG\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af53e15e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|summary|                ID|                DATE|                USER|                TEXT|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|  count|           1600000|             1600000|             1600000|             1600000|\n",
      "|   mean|          799999.5|                null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.35968924535|                null|5.162733218454889E10|                null|\n",
      "|    min|                 0|Fri Apr 17 20:30:...|        000catnap000|                 ...|\n",
      "|    max|           1599999|Wed May 27 07:27:...|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ca4568",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|summary|                ID|                DATE|                USER|                TEXT|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|  count|           1600000|             1600000|             1600000|             1600000|\n",
      "|   mean|          799999.5|                null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.35968924535|                null|5.162733218454889E10|                null|\n",
      "|    min|                 0|Fri Apr 17 20:30:...|        000catnap000|                 ...|\n",
      "|    25%|            399999|                null|             32508.0|                null|\n",
      "|    50%|            799999|                null|            130587.0|                null|\n",
      "|    75%|           1200076|                null|           1100101.0|                null|\n",
      "|    max|           1599999|Wed May 27 07:27:...|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a848ea0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|TEXT                                                                                                                                    |count|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|is poorly sick                                                                                                                          |4    |\n",
      "|This little tree is tiiiiired  25's (and dealing with stupid people) tomorrow and then finishing 10s monday! Raiding every nice         |2    |\n",
      "|at home with a cold                                                                                                                     |2    |\n",
      "|Hangover.                                                                                                                               |2    |\n",
      "|cant fall asleep                                                                                                                        |2    |\n",
      "|Confused                                                                                                                                |5    |\n",
      "|work all day                                                                                                                            |9    |\n",
      "|@tommcfly Arwhh, feel better soon (Y) JLC wont be the same tonight though  I am in food tech atm ;) weeeheeyy  have a lovely walk. x x x|2    |\n",
      "|marley and me is so sad                                                                                                                 |2    |\n",
      "|i have a fever                                                                                                                          |4    |\n",
      "|is not okay.                                                                                                                            |2    |\n",
      "|I want korean bbq so baddd but no one can come with me                                                                                  |2    |\n",
      "|lost my ipod                                                                                                                            |2    |\n",
      "|working all day                                                                                                                         |4    |\n",
      "|Exhausted.                                                                                                                              |4    |\n",
      "|missing my girls                                                                                                                        |2    |\n",
      "|soninho                                                                                                                                 |4    |\n",
      "|@dollforlife I don't know  , But i'll find a way !                                                                                      |2    |\n",
      "|@bngr Working m'dear...                                                                                                                 |2    |\n",
      "|I feel a cold coming on                                                                                                                 |2    |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Do a grouping and counting operation to find duplicate values in the \"TEXT\" column\n",
    "count_df = df.groupBy(\"TEXT\").count()\n",
    "\n",
    "# Filter rows containing duplicate values\n",
    "duplicate_values = count_df.filter(col(\"count\") > 1)\n",
    "\n",
    "# If there are duplicate values, show them\n",
    "if duplicate_values.count() > 0:\n",
    "    print(\"Duplicate values:\")\n",
    "    duplicate_values.show(truncate=False)  # Display column values in full length\n",
    "else:\n",
    "    print(\"No duplicate values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f12ca9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has 1600000 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78250158",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|DATE                        |\n",
      "+----------------------------+\n",
      "|Mon Apr 06 22:19:45 PDT 2009|\n",
      "|Mon Apr 06 22:19:49 PDT 2009|\n",
      "|Mon Apr 06 22:19:53 PDT 2009|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|\n",
      "|Mon Apr 06 22:20:00 PDT 2009|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|\n",
      "|Mon Apr 06 22:20:05 PDT 2009|\n",
      "|Mon Apr 06 22:20:09 PDT 2009|\n",
      "|Mon Apr 06 22:20:16 PDT 2009|\n",
      "|Mon Apr 06 22:20:17 PDT 2009|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|\n",
      "|Mon Apr 06 22:20:22 PDT 2009|\n",
      "|Mon Apr 06 22:20:25 PDT 2009|\n",
      "|Mon Apr 06 22:20:31 PDT 2009|\n",
      "|Mon Apr 06 22:20:34 PDT 2009|\n",
      "+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DATE\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62cf6457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- USER: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988dd02",
   "metadata": {},
   "source": [
    "## TEXT PRE-PROCESSING\n",
    "\n",
    "standard pre-processing techniques:\n",
    "\n",
    "- Lower casing the corpus \n",
    "- Removing the punctuation \n",
    "- Removing the stopwords \n",
    "- Tokenizing the corpus \n",
    "- Stemming and Lemmatization\n",
    "- Word embeddings using CountVectorizer and TF-IDF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd24a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                TEXT|\n",
      "+--------------------+\n",
      "|@switchfoot http:...|\n",
      "|is upset that he ...|\n",
      "|@Kenichan I dived...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = df.select(\"TEXT\")\n",
    "text_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e62f5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, lower, regexp_replace\n",
    "from pyspark.sql.types import StringType\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62dd2a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Temizlik fonksiyonu\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# UDF olarak kayıt\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Yeni kolon oluştur\n",
    "text_df = text_df.withColumn(\"TEXT_C1\", clean_text_udf(col(\"text\")))\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "104ba5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Kısaltmaları genişletecek fonksiyon\n",
    "def expand_contractions(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "# UDF olarak kaydet\n",
    "expand_contractions_udf = udf(expand_contractions, StringType())\n",
    "\n",
    "# DataFrame'de UDF'yi kullan\n",
    "text_df = text_df.withColumn(\"TEXT_C2\", expand_contractions_udf(col(\"TEXT_C1\")))\n",
    "\n",
    "# Sonuçları göster\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b970ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "punctuation_characters = r'[!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-./:;<=>\\?@[\\\\]\\^_`{|}~]'\n",
    "\n",
    "text_df = text_df.withColumn(\"TEXT_C3\", regexp_replace(col(\"TEXT_C2\"), punctuation_characters, \"\"))\n",
    "\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0f7eec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |TEXT_C4                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |bummer shoulda got david carr third day                                     |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|upset cannot update facebook texting might cry result school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |dived many times ball managed save 50 rest go bounds                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "remove_stopwords_udf = udf(lambda text: \" \".join([word for word in text.split() if word not in stop_words]), StringType())\n",
    "text_df = text_df.withColumn(\"TEXT_C4\", remove_stopwords_udf(col(\"TEXT_C3\")))\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66a6c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |TEXT_C4                                                                     |TEXT_C5                                                                      |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |bummer shoulda got david carr third day                                     |bummer shoulda got david carr third day                                      |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|upset cannot update facebook texting might cry result school today also blah|upset can not update facebook texting might cry result school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |dived many times ball managed save 50 rest go bounds                        |dived many time ball managed save 50 rest go bound                           |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# NLTK veri setlerini indirme\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# UDF olarak Lemmatization işlemini uygulayan fonksiyonu tanımlama\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w)\n",
    "        lemmatized_sentence.append(lemma)\n",
    "    lemmatized_text = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_text\n",
    "\n",
    "# UDF'yi tanımlama\n",
    "lemmatize_text_udf = udf(lemmatize_text, StringType())\n",
    "\n",
    "# DataFrame üzerinde UDF'yi kullanma\n",
    "text_df = text_df.withColumn(\"TEXT_C5\", lemmatize_text_udf(text_df[\"TEXT_C4\"]))\n",
    "text_df.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c676df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |TEXT_C4                                                                     |TEXT_C5                                                                      |TEXT_C6                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |bummer shoulda got david carr third day                                     |bummer shoulda got david carr third day                                      |bummer shoulda got david carr third day                                  |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|upset cannot update facebook texting might cry result school today also blah|upset can not update facebook texting might cry result school today also blah|upset can not updat facebook text might cri result school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |dived many times ball managed save 50 rest go bounds                        |dived many time ball managed save 50 rest go bound                           |dive mani time ball manag save 50 rest go bound                          |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# NLTK veri setlerini indirme\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# UDF olarak kelime köklerini bulan fonksiyonu tanımlama\n",
    "def stem_text(text):\n",
    "    snow = SnowballStemmer('english')\n",
    "    stemmed_sentence = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        stemmed_sentence.append(snow.stem(w))\n",
    "    stemmed_text = \" \".join(stemmed_sentence)\n",
    "    return stemmed_text\n",
    "\n",
    "# UDF'yi tanımlama\n",
    "stem_text_udf = udf(stem_text, StringType())\n",
    "\n",
    "# DataFrame üzerinde UDF'yi kullanma\n",
    "text_df = text_df.withColumn(\"TEXT_C6\", stem_text_udf(text_df[\"TEXT_C5\"]))\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8581422c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "|TEXT_C6                                                                  |tokens                                                                                 |\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "|bummer shoulda got david carr third day                                  |[bummer, shoulda, got, david, carr, third, day]                                        |\n",
      "|upset can not updat facebook text might cri result school today also blah|[upset, can, not, updat, facebook, text, might, cri, result, school, today, also, blah]|\n",
      "|dive mani time ball manag save 50 rest go bound                          |[dive, mani, time, ball, manag, save, 50, rest, go, bound]                             |\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# NLTK'yi kullanarak metni tokenlara bölen fonksiyon\n",
    "def tokenize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# UDF'yi tanımlama\n",
    "tokenize_text_udf = udf(tokenize_text, ArrayType(StringType()))\n",
    "\n",
    "# DataFrame üzerinde UDF'yi kullanma\n",
    "text_df = text_df.withColumn(\"tokens\", tokenize_text_udf(text_df[\"TEXT_C6\"]))\n",
    "\n",
    "# Sonuçları görüntüleme\n",
    "text_df.select(\"TEXT_C6\", \"tokens\").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9414ea0d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m943.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m724.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /home/hduser/.local/lib/python3.10/site-packages (from torch) (2023.10.0)\n",
      "Collecting triton==2.1.0\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/hduser/.local/lib/python3.10/site-packages (from torch) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions in /home/hduser/.local/lib/python3.10/site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch) (1.9)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "Successfully installed networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.52 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 triton-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "220ddf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e9c632d2fc4dc2a32750d5b90725b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'FloatProgress' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19018/2485801053.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# BERT tokenizer'ı yükle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# BERT modelini yükle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1938\u001b[0m                 \u001b[0;31m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m                 \u001b[0mfast_tokenizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFULL_TOKENIZER_FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   1941\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m                     \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1429\u001b[0m                     \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1432\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mdisplayed_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"(…){displayed_name[-20:]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m     progress = tqdm(\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/tqdm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mare_progress_bars_disabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"disable\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# Print initial bar state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mcolour\u001b[0;34m(self, bar_color)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FloatProgress' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# BERT tokenizer'ı yükle\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# BERT modelini yükle\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Modeli değerlendirme modunda kullanın\n",
    "model.eval()\n",
    "\n",
    "# UDF olarak duygu analizi yapacak fonksiyonu tanımlayın\n",
    "def analyze_sentiment(tokens):\n",
    "    # Tokenleri birleştirerek metin oluşturun\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    # Metni BERT için girişe dönüştürün\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Modeli kullanarak duygu analizi yapın\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Sonuçları pozitif, negatif ve nötr olarak etiketleyin\n",
    "    sentiment_labels = [\"Negatif\", \"Nötr\", \"Pozitif\"]\n",
    "    predicted_label = sentiment_labels[torch.argmax(logits, dim=1)]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# UDF'yi tanımlayın\n",
    "analyze_sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# DataFrame'deki \"tokens\" sütununu kullanarak duygu analizi yapın ve sonuçları yeni bir sütuna ekleyin\n",
    "text_df = text_df.withColumn(\"sentiment_result\", analyze_sentiment_udf(text_df[\"tokens\"]))\n",
    "\n",
    "# Sonuçları görüntüleyin\n",
    "text_df.select(\"tokens\", \"sentiment_result\").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a019d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e585c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ded37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8ecd15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e155554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62056097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "665a378e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:================================================>     (180 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| word| count|\n",
      "+-----+------+\n",
      "|   go|166488|\n",
      "|  get|109960|\n",
      "|  day|101499|\n",
      "| good| 90628|\n",
      "| work| 85203|\n",
      "| love| 83655|\n",
      "| like| 82963|\n",
      "| want| 73526|\n",
      "|  got| 70043|\n",
      "|today| 66095|\n",
      "|  can| 65259|\n",
      "|  not| 65157|\n",
      "| time| 64404|\n",
      "|thank| 60670|\n",
      "| know| 58230|\n",
      "| miss| 56620|\n",
      "| back| 56401|\n",
      "|  one| 55913|\n",
      "|  lol| 55388|\n",
      "|  see| 50987|\n",
      "+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:===================================================>  (192 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# SparkSession başlatılıyor\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Word Count\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Örnek veri yükleme (Sizin veri yükleme kodunuzu kullanın)\n",
    "# df = ...\n",
    "\n",
    "# Tokenizer oluşturuluyor\n",
    "tokenizer = Tokenizer(inputCol=\"TEXT_C6\", outputCol=\"words\")\n",
    "tokenizer_df = tokenizer.transform(text_df)\n",
    "\n",
    "# Kelimeleri tek tek satırlara ayırıyoruz\n",
    "tokenizer_df = tokenizer_df.select(explode(col(\"words\")).alias(\"word\"))\n",
    "\n",
    "# Kelimeleri sayıyoruz\n",
    "tokenizer_df_count = tokenizer_df.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# En çok kullanılan 50 kelimeyi alıyoruz\n",
    "tokenizer_df_count.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06c9d4ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   word|\n",
      "+-------+\n",
      "| bummer|\n",
      "|shoulda|\n",
      "|    got|\n",
      "+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "text_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00c03903",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"TEXT_C6\" among (word)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19018/2573384733.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mlabel_udf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_sentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtext_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TEXT_C6'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtext_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sentiment_label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \"\"\"\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"TEXT_C6\" among (word)"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Duygu analizi fonksiyonu\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# Duygu etiketi ekleme fonksiyonu\n",
    "def label_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'positive'\n",
    "    elif score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "# Spark DataFrame'i güncelle\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "\n",
    "sentiment_udf = udf(get_sentiment, FloatType())\n",
    "label_udf = udf(label_sentiment, StringType())\n",
    "\n",
    "text_df = text_df.withColumn('sentiment_score', sentiment_udf(text_df['TEXT_C6']))\n",
    "text_df = text_df.withColumn('sentiment_label', label_udf(text_df['sentiment_score']))\n",
    "\n",
    "# Etiket dağılımını göster\n",
    "text_df.groupBy('sentiment_label').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a41bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e6fff9",
   "metadata": {},
   "source": [
    "# TIME SERIES ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b99cb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
