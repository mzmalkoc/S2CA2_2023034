{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf5b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070968b",
   "metadata": {},
   "source": [
    "### CONNECTING TO MONGODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39694515",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mongo --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mongosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc29c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MongoDB\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DFToMongoDB\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Configure MongoDB Database Connection\n",
    "df.write.format(\"mongodb\") \\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1:27017/\") \\\n",
    "    .option(\"database\",\"sample_db\") \\\n",
    "    .option(\"collection\",\"scb\") \\\n",
    "    .mode(\"append\").save()\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e187f1",
   "metadata": {},
   "source": [
    "### CONNECTING TO MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mysql --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb6b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MySQL\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DFToMySQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure MySQL Database Connection\n",
    "mysql_options = {\n",
    "    \"url\": \"jdbc:mysql://localhost:3306/sample\",  # MySQL bağlantı URL'si\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",  # MySQL JDBC sürücüsü\n",
    "    \"dbtable\": \"yourtable\",  # Hedef MySQL tablo adı\n",
    "    \"user\": \"root\",  # MySQL kullanıcı adı\n",
    "    \"password\": \"password\"  # MySQL parola\n",
    "}\n",
    "\n",
    "# DataFrame'i MySQL veritabanına yükleyin\n",
    "df.write.format(\"jdbc\").options(**mysql_options).mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25733419",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat zahid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09369a9a",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# READ TO CSV FROM HDFS VIA SPARK\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae976e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"HDFSToCSV\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Specify CSV file path throught HDFS\n",
    "hdfs_file_path = \"/ProjectTweets.csv\"\n",
    "\n",
    "# Read CSV file with Spark DataFrame\n",
    "df = spark.read.csv(hdfs_file_path, header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f876af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show DataFrame First 5 Rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbe39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first method for renamed the column names\n",
    "df1 = df.withColumnRenamed(\"_c0\", \"id\").withColumnRenamed(\"_c1\", \"stamp\").withColumnRenamed(\"_c2\", \"date\").withColumnRenamed(\"_c3\", \"flag\").withColumnRenamed(\"_c4\", \"user\").withColumnRenamed(\"_c5\", \"text\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c856f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| ID|     STAMP|                DATE|    FLAG|           USER|                TEXT|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The second method for renamed the column names\n",
    "df = df.selectExpr(\"_c0 as ID\", \"_c1 as STAMP\", \"_c2 as DATE\", \"_c3 as FLAG\", \"_c4 as USER\", \"_c5 as TEXT\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ae87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae8b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"STAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count unique values\n",
    "for column in Columns:\n",
    "    unique_values = df.select(column).distinct()\n",
    "    unique_count = unique_values.count()\n",
    "    \n",
    "    if unique_count > 0:\n",
    "        print(f\"{column} has {unique_count} unique values:\")\n",
    "    else:\n",
    "        print(f\"{column} has no unique value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bbe08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"STAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count duplicate values\n",
    "for column in Columns:\n",
    "    count_df = df.groupBy(column).count()\n",
    "    duplicate_values = count_df.filter(col(\"count\") > 1).count()\n",
    "    \n",
    "    if duplicate_values > 0:\n",
    "        print(f\"{column} has {duplicate_values} duplicate values.\")\n",
    "    else:\n",
    "        print(f\"{column} has no duplicate value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2ebf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| ID|                DATE|                TEXT|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Mon Apr 06 22:19:...|@switchfoot http:...|\n",
      "|  1|Mon Apr 06 22:19:...|is upset that he ...|\n",
      "|  2|Mon Apr 06 22:19:...|@Kenichan I dived...|\n",
      "|  3|Mon Apr 06 22:19:...|my whole body fee...|\n",
      "|  4|Mon Apr 06 22:19:...|@nationwideclass ...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the selected columns\n",
    "df = df.drop(\"STAMP\", \"FLAG\", \"USER\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53e15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a848ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Do a grouping and counting operation to find duplicate values in the \"TEXT\" column\n",
    "count_df = df.groupBy(\"TEXT\").count()\n",
    "\n",
    "# Filter rows containing duplicate values\n",
    "duplicate_values = count_df.filter(col(\"count\") > 1)\n",
    "\n",
    "# If there are duplicate values, show them\n",
    "if duplicate_values.count() > 0:\n",
    "    print(\"Duplicate values:\")\n",
    "    duplicate_values.show(truncate=False)  # Display column values in full length\n",
    "else:\n",
    "    print(\"No duplicate values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f12ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988dd02",
   "metadata": {},
   "source": [
    "# =====================\n",
    "# TEXT PRE-PROCESSING\n",
    "# ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd24a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = df.select(\"TEXT\")\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e62f5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, lower, regexp_replace\n",
    "from pyspark.sql.types import ArrayType, StringType, FloatType\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "import torch\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8ac18",
   "metadata": {},
   "source": [
    "#### TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62dd2a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Save as UDF\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C1\", clean_text_udf(col(\"text\")))\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca560f3",
   "metadata": {},
   "source": [
    "#### EXPAND CONTRACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "104ba5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "# Save as UDF\n",
    "expand_contractions_udf = udf(expand_contractions, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C2\", expand_contractions_udf(col(\"TEXT_C1\")))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb546a",
   "metadata": {},
   "source": [
    "#### CLEAN THE PUNCTUATION CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b970ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define punctuation characters\n",
    "punctuation_characters = r'[!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-./:;<=>\\?@[\\\\]\\^_`{|}~]'\n",
    "\n",
    "# Remove punctuation characters\n",
    "text_df = text_df.withColumn(\"TEXT_C3\", regexp_replace(col(\"TEXT_C2\"), punctuation_characters, \"\"))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3af04d",
   "metadata": {},
   "source": [
    "#### CLEAN THE STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc0f7eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |TEXT_C4                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |bummer shoulda got david carr third day                                     |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|upset cannot update facebook texting might cry result school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |dived many times ball managed save 50 rest go bounds                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "# Download \"stopwords\" from nltk dictionary\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Configure the language as english\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define the udf \n",
    "remove_stopwords_udf = udf(lambda text: \" \".join([word for word in text.split() if word not in stop_words]), StringType())\n",
    "\n",
    "# Use the UDF in order to remove stopwords and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C4\", remove_stopwords_udf(col(\"TEXT_C3\")))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad341b",
   "metadata": {},
   "source": [
    "#### IMPLEMENT LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66a6c09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |TEXT_C4                                                                     |TEXT_C5                                                                      |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |bummer shoulda got david carr third day                                     |bummer shoulda got david carr third day                                      |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|upset cannot update facebook texting might cry result school today also blah|upset can not update facebook texting might cry result school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |dived many times ball managed save 50 rest go bounds                        |dived many time ball managed save 50 rest go bound                           |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download 'punkt','averaged_perceptron_tagger','wordnet' from nltk dictionary\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Defining the function that implements the Lemmatization operation as a UDF\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w)\n",
    "        lemmatized_sentence.append(lemma)\n",
    "    lemmatized_text = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Define the UDF\n",
    "lemmatize_text_udf = udf(lemmatize_text, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C5\", lemmatize_text_udf(text_df[\"TEXT_C4\"]))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932161f9",
   "metadata": {},
   "source": [
    "#### IMPLEMENT STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3c676df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |TEXT_C4                                                                     |TEXT_C5                                                                      |TEXT_C6                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |bummer shoulda got david carr third day                                     |bummer shoulda got david carr third day                                      |bummer shoulda got david carr third day                                  |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|upset cannot update facebook texting might cry result school today also blah|upset can not update facebook texting might cry result school today also blah|upset can not updat facebook text might cri result school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |dived many times ball managed save 50 rest go bounds                        |dived many time ball managed save 50 rest go bound                           |dive mani time ball manag save 50 rest go bound                          |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Defining the function that finds word roots as UDF \n",
    "def stem_text(text):\n",
    "    snow = SnowballStemmer('english')\n",
    "    stemmed_sentence = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        stemmed_sentence.append(snow.stem(w))\n",
    "    stemmed_text = \" \".join(stemmed_sentence)\n",
    "    return stemmed_text\n",
    "\n",
    "# Define the UDF\n",
    "stem_text_udf = udf(stem_text, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C6\", stem_text_udf(text_df[\"TEXT_C5\"]))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e4dc6",
   "metadata": {},
   "source": [
    "#### IMPLEMENT TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8581422c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "|TEXT_C6                                                                  |tokens                                                                                 |\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "|bummer shoulda got david carr third day                                  |[bummer, shoulda, got, david, carr, third, day]                                        |\n",
      "|upset can not updat facebook text might cri result school today also blah|[upset, can, not, updat, facebook, text, might, cri, result, school, today, also, blah]|\n",
      "|dive mani time ball manag save 50 rest go bound                          |[dive, mani, time, ball, manag, save, 50, rest, go, bound]                             |\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function that splits text into tokens using NLTK\n",
    "def tokenize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# Define the UDF\n",
    "tokenize_text_udf = udf(tokenize_text, ArrayType(StringType()))\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"tokens\", tokenize_text_udf(text_df[\"TEXT_C6\"]))\n",
    "\n",
    "# Show the selected dataframe\n",
    "text_df.select(\"TEXT_C6\", \"tokens\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e720fc7",
   "metadata": {},
   "source": [
    "#### IMPLEMENT TOKENIZATION AND SPLIT WORDS TO ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a378e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Create The Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"TEXT_C6\", outputCol=\"words\")\n",
    "tokenizer_df = tokenizer.transform(text_df)\n",
    "\n",
    "# Separate words into individual lines\n",
    "tokenizer_df = tokenizer_df.select(explode(col(\"words\")).alias(\"word\"))\n",
    "\n",
    "# Show the dataframe\n",
    "tokenizer_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f10c9",
   "metadata": {},
   "source": [
    "#### COUNT THE TOKENIZER WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace1722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the tokenizer words\n",
    "tokenizer_df_count = tokenizer_df.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Show the dataframe\n",
    "tokenizer_df_count.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf963ad",
   "metadata": {},
   "source": [
    "#### SENTIMENT LABEL ( POSITIVE - NEGATIVE - NEUTRAL )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00c03903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|sentiment_label| count|\n",
      "+---------------+------+\n",
      "|       positive|582066|\n",
      "|        neutral|782520|\n",
      "|       negative|235414|\n",
      "+---------------+------+\n",
      "\n",
      "CPU times: user 112 ms, sys: 22.3 ms, total: 134 ms\n",
      "Wall time: 23min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:===============================================>        (64 + 1) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sentimental Analysis Function\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# Sentiment Label Function\n",
    "def label_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'positive'\n",
    "    elif score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "#=========================================================================================#\n",
    "\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "\n",
    "# Define the UDF with Functions\n",
    "sentiment_udf = udf(get_sentiment, FloatType())\n",
    "label_udf = udf(label_sentiment, StringType())\n",
    "\n",
    "# Use the UDF and Create new columns\n",
    "text_df = text_df.withColumn('sentiment_score', sentiment_udf(text_df['TEXT_C6']))\n",
    "text_df = text_df.withColumn('sentiment_label', label_udf(text_df['sentiment_score']))\n",
    "\n",
    "# Count and Show the 'sentiment_label' column\n",
    "text_df.groupBy('sentiment_label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e6cb8",
   "metadata": {},
   "source": [
    "#### The Positive, Neutral and Negative Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b3de32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rate: 36.379125%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count the whole text \n",
    "total_count = text_df.count()\n",
    "# Count the positive sentiment label\n",
    "positive_count = text_df.filter(text_df.sentiment_label == \"positive\").count()\n",
    "# Calculate the positive rate\n",
    "positive_rate = (positive_count / total_count) * 100\n",
    "# Print the positive rate\n",
    "print(f\"Positive rate: {positive_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207fb7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the whole text \n",
    "total_count = text_df.count()\n",
    "# Count the neutral sentiment label\n",
    "neutral_count = text_df.filter(text_df.sentiment_label == \"neutral\").count()\n",
    "# Calculate the neutral rate\n",
    "neutral_rate = (neutral_count / total_count) * 100\n",
    "# Print the neutral rate\n",
    "print(f\"Neutral rate: {neutral_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the whole text \n",
    "total_count = text_df.count()\n",
    "# Count the negative sentiment label\n",
    "negative_count = text_df.filter(text_df.sentiment_label == \"negative\").count()\n",
    "# Calculate the negative rate\n",
    "negative_rate = (negative_count / total_count) * 100\n",
    "# Print the negative rate\n",
    "print(f\"Negative rate: {negative_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88cb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the positive, neutral and negative rate\n",
    "print(f\"Positive rate: {positive_rate}%\")\n",
    "print(f\"Neutral rate: {neutral_rate}%\")\n",
    "print(f\"Negative rate: {negative_rate}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e981a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ea7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Duygu etiketlerini yeni bir sütunda saklama\n",
    "text_df = text_df.withColumn(\"sentiment_label_column\", \n",
    "     when(text_df[\"sentiment_label\"] == \"positive\", \"positive\")\n",
    "    .when(text_df[\"sentiment_label\"] == \"neutral\", \"neutral\")\n",
    "    .when(text_df[\"sentiment_label\"] == \"negative\", \"negative\")\n",
    "    .otherwise(\"unknown\")\n",
    ")\n",
    "\n",
    "# Show the selected columns\n",
    "text_df.select(\"TEXT_C6\", \"sentiment_label_column\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36eb1ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+--------------+\n",
      "|TEXT_C6                                                                  |polarity_score|\n",
      "+-------------------------------------------------------------------------+--------------+\n",
      "|bummer shoulda got david carr third day                                  |0.0           |\n",
      "|upset can not updat facebook text might cri result school today also blah|0.0           |\n",
      "|dive mani time ball manag save 50 rest go bound                          |0.0           |\n",
      "|whole bodi feel itchi like fire                                          |0.2           |\n",
      "|behav mad can not see                                                    |-0.625        |\n",
      "|whole crew                                                               |0.2           |\n",
      "|need hug                                                                 |0.0           |\n",
      "|hey long time see yes rain bit bit lol fine thank                        |0.3888889     |\n",
      "|nope                                                                     |0.0           |\n",
      "|que muera                                                                |0.0           |\n",
      "|spring break plain citi snow                                             |-0.21428572   |\n",
      "|repierc ear                                                              |0.0           |\n",
      "|could bear watch thought ua loss embarrass                               |0.0           |\n",
      "|count know either never talk anymor                                      |0.0           |\n",
      "|would first gun realli though zac snyder doucheclown                     |0.25          |\n",
      "|wish got watch miss premier                                              |0.0           |\n",
      "|holli death scene hurt sever watch film wri director cut                 |0.0           |\n",
      "|file tax                                                                 |0.0           |\n",
      "|ahh alway want see rent love soundtrack                                  |0.5           |\n",
      "|oh dear drink forgotten tabl drink                                       |0.0           |\n",
      "+-------------------------------------------------------------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 6.11 ms, sys: 944 µs, total: 7.05 ms\n",
      "Wall time: 12.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Function that measures polarity\n",
    "def get_polarity(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity \n",
    "\n",
    "# Define UDF\n",
    "polarity_udf = udf(get_polarity, FloatType())\n",
    "\n",
    "# Calculate polarity score for each text in column 'TEXT_C6' and add to a new column\n",
    "text_df = text_df.withColumn(\"polarity_score\", polarity_udf(text_df['TEXT_C6']))\n",
    "\n",
    "# Show the new column\n",
    "text_df.select(\"TEXT_C6\", \"polarity_score\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23cd6d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 30:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------------+---------------+\n",
      "|                TEXT|             TEXT_C6|sentiment_label|sentiment_score|\n",
      "+--------------------+--------------------+---------------+---------------+\n",
      "|@switchfoot http:...|bummer shoulda go...|        neutral|            0.0|\n",
      "|is upset that he ...|upset can not upd...|        neutral|            0.0|\n",
      "|@Kenichan I dived...|dive mani time ba...|        neutral|            0.0|\n",
      "|my whole body fee...|whole bodi feel i...|       positive|            0.2|\n",
      "|@nationwideclass ...|behav mad can not...|       negative|         -0.625|\n",
      "+--------------------+--------------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the new column\n",
    "text_df = text_df.select('TEXT','TEXT_C6', 'sentiment_label', 'sentiment_score').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e479bad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4107/285589061.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "text_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46eb7e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| ID|                DATE|                TEXT|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Mon Apr 06 22:19:...|@switchfoot http:...|\n",
      "|  1|Mon Apr 06 22:19:...|is upset that he ...|\n",
      "|  2|Mon Apr 06 22:19:...|@Kenichan I dived...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a20e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f1876c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101e6fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e6fff9",
   "metadata": {},
   "source": [
    "# ===============================================\n",
    "# TIMESTAMP PREPARATION FOR TIME SERIES ANALYSIS\n",
    "# ================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "78250158",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 72:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|DATE                        |TEXT                                                                                                                 |\n",
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "|Mon Apr 06 22:19:45 PDT 2009|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |\n",
      "|Mon Apr 06 22:19:49 PDT 2009|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |\n",
      "|Mon Apr 06 22:19:53 PDT 2009|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |\n",
      "|Mon Apr 06 22:19:57 PDT 2009|my whole body feels itchy and like its on fire                                                                       |\n",
      "|Mon Apr 06 22:19:57 PDT 2009|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |\n",
      "|Mon Apr 06 22:20:00 PDT 2009|@Kwesidei not the whole crew                                                                                         |\n",
      "|Mon Apr 06 22:20:03 PDT 2009|Need a hug                                                                                                           |\n",
      "|Mon Apr 06 22:20:03 PDT 2009|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |\n",
      "|Mon Apr 06 22:20:05 PDT 2009|@Tatiana_K nope they didn't have it                                                                                  |\n",
      "|Mon Apr 06 22:20:09 PDT 2009|@twittera que me muera ?                                                                                             |\n",
      "|Mon Apr 06 22:20:16 PDT 2009|spring break in plain city... it's snowing                                                                           |\n",
      "|Mon Apr 06 22:20:17 PDT 2009|I just re-pierced my ears                                                                                            |\n",
      "|Mon Apr 06 22:20:19 PDT 2009|@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |\n",
      "|Mon Apr 06 22:20:19 PDT 2009|@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |\n",
      "|Mon Apr 06 22:20:20 PDT 2009|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |\n",
      "|Mon Apr 06 22:20:22 PDT 2009|Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |\n",
      "|Mon Apr 06 22:20:25 PDT 2009|about to file taxes                                                                                                  |\n",
      "|Mon Apr 06 22:20:31 PDT 2009|@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |\n",
      "|Mon Apr 06 22:20:34 PDT 2009|@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |\n",
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Select the DATE column\n",
    "date_df = df.select(\"DATE\", \"TEXT\")\n",
    "\n",
    "# Show the DATE column\n",
    "date_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "efc6f3eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the schema of the dataframe\n",
    "date_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "366f5d8d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 73:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------+\n",
      "|DATE                        |TIMESTAMP          |\n",
      "+----------------------------+-------------------+\n",
      "|Mon Apr 06 22:19:45 PDT 2009|2009-04-07 06:19:45|\n",
      "|Mon Apr 06 22:19:49 PDT 2009|2009-04-07 06:19:49|\n",
      "|Mon Apr 06 22:19:53 PDT 2009|2009-04-07 06:19:53|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|2009-04-07 06:19:57|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|2009-04-07 06:19:57|\n",
      "|Mon Apr 06 22:20:00 PDT 2009|2009-04-07 06:20:00|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|2009-04-07 06:20:03|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|2009-04-07 06:20:03|\n",
      "|Mon Apr 06 22:20:05 PDT 2009|2009-04-07 06:20:05|\n",
      "|Mon Apr 06 22:20:09 PDT 2009|2009-04-07 06:20:09|\n",
      "|Mon Apr 06 22:20:16 PDT 2009|2009-04-07 06:20:16|\n",
      "|Mon Apr 06 22:20:17 PDT 2009|2009-04-07 06:20:17|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|2009-04-07 06:20:19|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|2009-04-07 06:20:19|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|2009-04-07 06:20:20|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|2009-04-07 06:20:20|\n",
      "|Mon Apr 06 22:20:22 PDT 2009|2009-04-07 06:20:22|\n",
      "|Mon Apr 06 22:20:25 PDT 2009|2009-04-07 06:20:25|\n",
      "|Mon Apr 06 22:20:31 PDT 2009|2009-04-07 06:20:31|\n",
      "|Mon Apr 06 22:20:34 PDT 2009|2009-04-07 06:20:34|\n",
      "+----------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "# Convert to 'yyyy-MM-dd HH:mm:ss' format\n",
    "date_df = date_df.withColumn(\"TIMESTAMP\", to_timestamp(col(\"DATE\"), \"EEE MMM dd HH:mm:ss zzz yyyy\"))\n",
    "\n",
    "# Show the selected columns\n",
    "date_df.select(\"DATE\", \"TIMESTAMP\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3d31567",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 74:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n",
      "|TIMESTAMP          |YearMonthDate|\n",
      "+-------------------+-------------+\n",
      "|2009-04-07 06:19:45|2009-04-07   |\n",
      "|2009-04-07 06:19:49|2009-04-07   |\n",
      "|2009-04-07 06:19:53|2009-04-07   |\n",
      "|2009-04-07 06:19:57|2009-04-07   |\n",
      "|2009-04-07 06:19:57|2009-04-07   |\n",
      "|2009-04-07 06:20:00|2009-04-07   |\n",
      "|2009-04-07 06:20:03|2009-04-07   |\n",
      "|2009-04-07 06:20:03|2009-04-07   |\n",
      "|2009-04-07 06:20:05|2009-04-07   |\n",
      "|2009-04-07 06:20:09|2009-04-07   |\n",
      "|2009-04-07 06:20:16|2009-04-07   |\n",
      "|2009-04-07 06:20:17|2009-04-07   |\n",
      "|2009-04-07 06:20:19|2009-04-07   |\n",
      "|2009-04-07 06:20:19|2009-04-07   |\n",
      "|2009-04-07 06:20:20|2009-04-07   |\n",
      "|2009-04-07 06:20:20|2009-04-07   |\n",
      "|2009-04-07 06:20:22|2009-04-07   |\n",
      "|2009-04-07 06:20:25|2009-04-07   |\n",
      "|2009-04-07 06:20:31|2009-04-07   |\n",
      "|2009-04-07 06:20:34|2009-04-07   |\n",
      "+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# COnvert to 'yyyy-MM-dd' format\n",
    "date_df = date_df.withColumn(\"YearMonthDate\", col(\"TIMESTAMP\").substr(1, 10))\n",
    "\n",
    "# Show the selected columns\n",
    "date_df.select(\"TIMESTAMP\", \"YearMonthDate\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894ec7b",
   "metadata": {},
   "source": [
    "# B U N A     -     B I      -    B A K\n",
    "### EKSIK GUNLERI NASIL BULACAGIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea0f501",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663b17b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e5cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74df3db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8185e2b3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|DATE                        |TEXT                                                                                                                 |TIMESTAMP          |\n",
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|Mon Apr 06 22:19:45 PDT 2009|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  |2009-04-07 06:19:45|\n",
      "|Mon Apr 06 22:19:49 PDT 2009|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!      |2009-04-07 06:19:49|\n",
      "|Mon Apr 06 22:19:53 PDT 2009|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                            |2009-04-07 06:19:53|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|my whole body feels itchy and like its on fire                                                                       |2009-04-07 06:19:57|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|@nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.       |2009-04-07 06:19:57|\n",
      "|Mon Apr 06 22:20:00 PDT 2009|@Kwesidei not the whole crew                                                                                         |2009-04-07 06:20:00|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|Need a hug                                                                                                           |2009-04-07 06:20:03|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|@LOLTrish hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?                  |2009-04-07 06:20:03|\n",
      "|Mon Apr 06 22:20:05 PDT 2009|@Tatiana_K nope they didn't have it                                                                                  |2009-04-07 06:20:05|\n",
      "|Mon Apr 06 22:20:09 PDT 2009|@twittera que me muera ?                                                                                             |2009-04-07 06:20:09|\n",
      "|Mon Apr 06 22:20:16 PDT 2009|spring break in plain city... it's snowing                                                                           |2009-04-07 06:20:16|\n",
      "|Mon Apr 06 22:20:17 PDT 2009|I just re-pierced my ears                                                                                            |2009-04-07 06:20:17|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|@caregiving I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .                       |2009-04-07 06:20:19|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|@octolinz16 It it counts, idk why I did either. you never talk to me anymore                                         |2009-04-07 06:20:19|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|@smarrison i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.|2009-04-07 06:20:20|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|@iamjazzyfizzle I wish I got to watch it with you!! I miss you and @iamlilnicki  how was the premiere?!              |2009-04-07 06:20:20|\n",
      "|Mon Apr 06 22:20:22 PDT 2009|Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?                        |2009-04-07 06:20:22|\n",
      "|Mon Apr 06 22:20:25 PDT 2009|about to file taxes                                                                                                  |2009-04-07 06:20:25|\n",
      "|Mon Apr 06 22:20:31 PDT 2009|@LettyA ahh ive always wanted to see rent  love the soundtrack!!                                                     |2009-04-07 06:20:31|\n",
      "|Mon Apr 06 22:20:34 PDT 2009|@FakerPattyPattz Oh dear. Were you drinking out of the forgotten table drinks?                                       |2009-04-07 06:20:34|\n",
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Sort ascending via TIMESTAMP column\n",
    "date_df = date_df.orderBy(\"TIMESTAMP\", ascending=True)\n",
    "\n",
    "# Show sorted DataFrame\n",
    "date_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select THE NEWEST DATE\n",
    "newest_date = date_df.select(\"TIMESTAMP\").first()[0]\n",
    "\n",
    "# Print THE NEWEST DATE\n",
    "print(\"THE NEWEST DATE:\", newest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6568a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select THE OLDEST DATE\n",
    "oldest_date = date_df.select(\"TIMESTAMP\").collect()[-1][0]\n",
    "\n",
    "# Print THE OLDEST DATE\n",
    "print(\"THE OLDEST DATE:\", oldest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455645e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ba54f9",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# MYSQL CONNECTION AND YCSB TEST\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa1d4b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MySQL\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DFToMySQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configure MySQL Database Connection\n",
    "mysql_options = {\n",
    "    \"url\": \"jdbc:mysql://localhost:3306/sample\",  # MySQL bağlantı URL'si\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",  # MySQL JDBC sürücüsü\n",
    "    \"dbtable\": \"yourtable\",  # Hedef MySQL tablo adı\n",
    "    \"user\": \"root\",  # MySQL kullanıcı adı\n",
    "    \"password\": \"password\"  # MySQL parola\n",
    "}\n",
    "\n",
    "# DataFrame'i MySQL veritabanına yükleyin\n",
    "df.write.format(\"jdbc\").options(**mysql_options).mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65f5d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.jdbc(url=\"jdbc:mysql://localhost:3306/sample\", table=\"yourtable\", properties=mysql_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d217c9ce",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| ID|                DATE|                TEXT|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Mon Apr 06 22:19:...|@switchfoot http:...|\n",
      "|  1|Mon Apr 06 22:19:...|is upset that he ...|\n",
      "|  2|Mon Apr 06 22:19:...|@Kenichan I dived...|\n",
      "|  3|Mon Apr 06 22:19:...|my whole body fee...|\n",
      "|  4|Mon Apr 06 22:19:...|@nationwideclass ...|\n",
      "|  5|Mon Apr 06 22:20:...|@Kwesidei not the...|\n",
      "|  6|Mon Apr 06 22:20:...|         Need a hug |\n",
      "|  7|Mon Apr 06 22:20:...|@LOLTrish hey  lo...|\n",
      "|  8|Mon Apr 06 22:20:...|@Tatiana_K nope t...|\n",
      "|  9|Mon Apr 06 22:20:...|@twittera que me ...|\n",
      "| 10|Mon Apr 06 22:20:...|spring break in p...|\n",
      "| 11|Mon Apr 06 22:20:...|I just re-pierced...|\n",
      "| 12|Mon Apr 06 22:20:...|@caregiving I cou...|\n",
      "| 13|Mon Apr 06 22:20:...|@octolinz16 It it...|\n",
      "| 14|Mon Apr 06 22:20:...|@smarrison i woul...|\n",
      "| 15|Mon Apr 06 22:20:...|@iamjazzyfizzle I...|\n",
      "| 16|Mon Apr 06 22:20:...|Hollis' death sce...|\n",
      "| 17|Mon Apr 06 22:20:...|about to file taxes |\n",
      "| 18|Mon Apr 06 22:20:...|@LettyA ahh ive a...|\n",
      "| 19|Mon Apr 06 22:20:...|@FakerPattyPattz ...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d517788",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/java  -classpath /home/hduser/ycsb-0.17.0/conf:/home/hduser/ycsb-0.17.0/lib/HdrHistogram-2.1.4.jar:/home/hduser/ycsb-0.17.0/lib/core-0.17.0.jar:/home/hduser/ycsb-0.17.0/lib/htrace-core4-4.1.0-incubating.jar:/home/hduser/ycsb-0.17.0/lib/jackson-core-asl-1.9.4.jar:/home/hduser/ycsb-0.17.0/lib/jackson-mapper-asl-1.9.4.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/conf:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/commons-collections-3.2.1.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/commons-lang-2.4.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/commons-pool-1.5.4.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/geronimo-jms_1.1_spec-1.1.1.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/geronimo-jta_1.1_spec-1.1.1.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/jdbc-binding-0.17.0.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/mysql-connector-java-8.0.30.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/openjpa-jdbc-2.1.1.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/openjpa-kernel-2.1.1.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/openjpa-lib-2.1.1.jar:/home/hduser/ycsb-0.17.0/jdbc-binding/lib/serp-1.13.1.jar site.ycsb.Client -t -db site.ycsb.db.JdbcDBClient -P /home/hduser/ycsb-0.17.0/workloads/workloada -p db.url=jdbc:mysql://localhost:3306/tweet_mysql -p db.user=root -p db.passwd=kalem -p db.driver=com.mysql.cj.jdbc.Driver\n",
      "Command line: -t -db site.ycsb.db.JdbcDBClient -P /home/hduser/ycsb-0.17.0/workloads/workloada -p db.url=jdbc:mysql://localhost:3306/tweet_mysql -p db.user=root -p db.passwd=kalem -p db.driver=com.mysql.cj.jdbc.Driver\n",
      "YCSB Client 0.17.0\n",
      "\n",
      "Loading workload...\n",
      "Starting test.\n",
      "Adding shard node URL: jdbc:mysql://localhost:3306/tweet_mysql\n",
      "Error in database operation: java.sql.SQLException: Access denied for user 'root'@'localhost' (using password: YES)\n",
      "site.ycsb.DBException: java.sql.SQLException: Access denied for user 'root'@'localhost' (using password: YES)\n",
      "\tat site.ycsb.db.JdbcDBClient.init(JdbcDBClient.java:231)\n",
      "\tat site.ycsb.DBWrapper.init(DBWrapper.java:86)\n",
      "\tat site.ycsb.ClientThread.run(ClientThread.java:91)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.sql.SQLException: Access denied for user 'root'@'localhost' (using password: YES)\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n",
      "\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:828)\n",
      "\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:448)\n",
      "\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:241)\n",
      "\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:198)\n",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:247)\n",
      "\tat site.ycsb.db.JdbcDBClient.init(JdbcDBClient.java:209)\n",
      "\t... 3 more\n",
      "site.ycsb.DBException: java.sql.SQLException: Access denied for user 'root'@'localhost' (using password: YES)\n",
      "\tat site.ycsb.db.JdbcDBClient.init(JdbcDBClient.java:231)\n",
      "\tat site.ycsb.DBWrapper.init(DBWrapper.java:86)\n",
      "\tat site.ycsb.ClientThread.run(ClientThread.java:91)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.sql.SQLException: Access denied for user 'root'@'localhost' (using password: YES)\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129)\n",
      "\tat com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122)\n",
      "\tat com.mysql.cj.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:828)\n",
      "\tat com.mysql.cj.jdbc.ConnectionImpl.<init>(ConnectionImpl.java:448)\n",
      "\tat com.mysql.cj.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:241)\n",
      "\tat com.mysql.cj.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:198)\n",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n",
      "\tat java.sql.DriverManager.getConnection(DriverManager.java:247)\n",
      "\tat site.ycsb.db.JdbcDBClient.init(JdbcDBClient.java:209)\n",
      "\t... 3 more\n",
      "[OVERALL], RunTime(ms), 1635\n",
      "[OVERALL], Throughput(ops/sec), 0.0\n",
      "[TOTAL_GCS_Copy], Count, 1\n",
      "[TOTAL_GC_TIME_Copy], Time(ms), 8\n",
      "[TOTAL_GC_TIME_%_Copy], Time(%), 0.4892966360856269\n",
      "[TOTAL_GCS_MarkSweepCompact], Count, 0\n",
      "[TOTAL_GC_TIME_MarkSweepCompact], Time(ms), 0\n",
      "[TOTAL_GC_TIME_%_MarkSweepCompact], Time(%), 0.0\n",
      "[TOTAL_GCs], Count, 1\n",
      "[TOTAL_GC_TIME], Time(ms), 8\n",
      "[TOTAL_GC_TIME_%], Time(%), 0.4892966360856269\n"
     ]
    }
   ],
   "source": [
    "# YCSB TEST FOR MYSQL\n",
    "!/home/hduser/ycsb-0.17.0/bin/ycsb.sh run jdbc -P /home/hduser/ycsb-0.17.0/workloads/workloada -p db.url=jdbc:mysql://localhost:3306/tweet_mysql -p db.user=root -p db.passwd=kalem -p db.driver=com.mysql.cj.jdbc.Driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb747f30",
   "metadata": {},
   "source": [
    "# ===================================\n",
    "# MONGODB CONNECTION AND YCSB TEST\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e0050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee047410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YCSB TEST FOR MONGODB\n",
    "!/home/hduser/ycsb-0.17.0/bin/ycsb.sh run mongodb -P /home/hduser/ycsb-0.17.0/workloads/workloada -p mongodb.url=mongodb://localhost:27017 -p mongodb.database=tweet_mongo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da9a7da",
   "metadata": {},
   "source": [
    "# ==========================\n",
    "# DEEP LEARNING - RNN MODEL\n",
    "# =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d854fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "680f30cd",
   "metadata": {},
   "source": [
    "# ======\n",
    "## GRAPHS\n",
    "# ======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02774035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# 1-Week Analysis\n",
    "weekly_data = date_df.groupBy(date_format(\"TIMESTAMP\", \"yyyy-ww\")).count()\n",
    "weekly_data = weekly_data.withColumnRenamed(\"date_format(TIMESTAMP, yyyy-ww)\", \"week\")\n",
    "weekly_data = weekly_data.orderBy(\"week\", ascending=True)\n",
    "weekly_data.show()\n",
    "\n",
    "# Get the result and visualize it\n",
    "weekly_data_pd = weekly_data.toPandas()\n",
    "\n",
    "# Plot for 1-Week Time Series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(weekly_data_pd[\"week\"], weekly_data_pd[\"count\"], width=0.5)\n",
    "plt.title(\"Weekly Tweet Count\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa49fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# 1-Month Analysis\n",
    "monthly_data = date_df.groupBy(date_format(\"TIMESTAMP\", \"yyyy-MM\")).count()\n",
    "monthly_data = monthly_data.withColumnRenamed(\"date_format(TIMESTAMP, yyyy-MM)\", \"month\")\n",
    "monthly_data.show()\n",
    "\n",
    "# Get the results and visualize it\n",
    "monthly_data_pd = monthly_data.toPandas()\n",
    "\n",
    "# Plot for 1-Month Time Series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(monthly_data_pd[\"month\"], monthly_data_pd[\"count\"], width=0.5)\n",
    "plt.title(\"Monthly Tweet Count\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "# 3-Month Analysis\n",
    "quarterly_data = date_df.groupBy(date_format(\"TIMESTAMP\", \"yyyy-MM\")).count()\n",
    "quarterly_data = quarterly_data.withColumnRenamed(\"date_format(TIMESTAMP, yyyy-MM)\", \"quarter\")\n",
    "quarterly_data.show()\n",
    "\n",
    "# Get the results and visualize it\n",
    "quarterly_data_pd = quarterly_data.toPandas()\n",
    "\n",
    "# Plot for 3-Month Time Series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(quarterly_data_pd[\"quarter\"], quarterly_data_pd[\"count\"], width=0.5)\n",
    "plt.title(\"3-Month Tweet Count\")\n",
    "plt.xlabel(\"Quarter\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boş değerleri kontrol etmek için isNull() kullanın\n",
    "nan_value = date_df.filter(F.col(\"TIMESTAMP\").isNull())\n",
    "\n",
    "# Hangi satırlarda boş değerler olduğunu gösterin\n",
    "print(\"Boş değerlerin olduğu satırlar:\")\n",
    "nan_value.show()\n",
    "\n",
    "# Toplam boş değer sayısını alın\n",
    "nan_value_count = nan_value.count()\n",
    "print(\"Toplam boş değer sayısı:\", nan_value_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e51f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
