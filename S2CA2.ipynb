{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf5b36e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070968b",
   "metadata": {},
   "source": [
    "### CONNECTING TO MONGODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39694515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB shell version: 3.2.10\r\n"
     ]
    }
   ],
   "source": [
    "!mongo --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mongosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc29c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MongoDB\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DFToMongoDB\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# # #\n",
    "data = [(\"John\", 28), (\"Alice\", 22), (\"Bob\", 32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c80aada",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| John| 28|\n",
      "|Alice| 22|\n",
      "|  Bob| 32|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:===========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f56ac60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Configure MongoDB Database Connection\n",
    "df.write.format(\"mongodb\") \\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1:27017/\") \\\n",
    "    .option(\"database\",\"sample_db\") \\\n",
    "    .option(\"collection\",\"scb\") \\\n",
    "    .mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e187f1",
   "metadata": {},
   "source": [
    "### CONNECTING TO MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb34813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql  Ver 8.0.30-0ubuntu0.22.04.1 for Linux on x86_64 ((Ubuntu))\r\n"
     ]
    }
   ],
   "source": [
    "!mysql --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edb6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MySQL\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DFToMySQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# # #\n",
    "data = [(\"John\", 28), (\"Alice\", 22), (\"Bob\", 32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Configure MySQL Database Connection\n",
    "mysql_options = {\n",
    "    \"url\": \"jdbc:mysql://localhost:3306/sample\",  # MySQL bağlantı URL'si\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",  # MySQL JDBC sürücüsü\n",
    "    \"dbtable\": \"yourtable\",  # Hedef MySQL tablo adı\n",
    "    \"user\": \"root\",  # MySQL kullanıcı adı\n",
    "    \"password\": \"password\"  # MySQL parola\n",
    "}\n",
    "\n",
    "# DataFrame'i MySQL veritabanına yükleyin\n",
    "df.write.format(\"jdbc\").options(**mysql_options).mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25733419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hduser/Desktop\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3879a6cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwd\r\n",
      "cd Downloads/\r\n",
      "nano zahid.txt\r\n",
      "cat zahid.txt \r\n",
      "-----------------------------------------------\r\n",
      "mysql -u root -p\r\n",
      "Enter password: password\r\n",
      "CREATE DATABASE sample;\r\n",
      "USE sample;\r\n",
      "SHOW TABLES;\r\n",
      "-----------------------------------------------\r\n",
      "Downloads$ ls mon*\r\n",
      "Downloads$ sudo cp mon* /usr/local/spark/jars\r\n",
      "Downloads$ sudo cp ./bson-3.12.12.jar /usr/local/spark/jars\r\n",
      "Downloads$ ls mys*\r\n",
      "Downloads$ sudo cp ./mysql-connector-j-8.0.33.jar /usr/local/spark/jars\r\n",
      "--------------------------------------------------------------------------\r\n",
      "cd /usr/local/spark/jars\r\n",
      "/usr/local/spark/jars$ ls mon*\r\n",
      "/usr/local/spark/jars$ ls mysql*\r\n",
      "--------------------------------------------------------------------------\r\n",
      "mongosh\r\n",
      "/mongodb-linux-x86_64-ubuntu1604-3.2.10$ ./bin/mongod\r\n",
      "--------------------------------------------------------------------------\r\n",
      "https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/3.12.12/\r\n",
      "--------------------------------------------------------------------------\r\n",
      "mysql --version\r\n",
      "mongo --version\r\n"
     ]
    }
   ],
   "source": [
    "!cat zahid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09369a9a",
   "metadata": {},
   "source": [
    "# ================================\n",
    "# READ TO CSV FROM HDFS VIA SPARK\n",
    "# ================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae976e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"HDFSToCSV\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Specify CSV file path throught HDFS\n",
    "hdfs_file_path = \"/ProjectTweets.csv\"\n",
    "\n",
    "# Read CSV file with Spark DataFrame\n",
    "df = spark.read.csv(hdfs_file_path, header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f876af7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show DataFrame First 5 Rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74e9815",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdbe39f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| id| timestamp|                date|    flag|           user|                text|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The first method for renamed the column names\n",
    "df1 = df.withColumnRenamed(\"_c0\", \"id\").withColumnRenamed(\"_c1\", \"timestamp\").withColumnRenamed(\"_c2\", \"date\").withColumnRenamed(\"_c3\", \"flag\").withColumnRenamed(\"_c4\", \"user\").withColumnRenamed(\"_c5\", \"text\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c856f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| ID| TIMESTAMP|                DATE|    FLAG|           USER|                TEXT|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The second method for renamed the column names\n",
    "df = df.selectExpr(\"_c0 as ID\", \"_c1 as TIMESTAMP\", \"_c2 as DATE\", \"_c3 as FLAG\", \"_c4 as USER\", \"_c5 as TEXT\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55ae87a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has 1600000 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ae8b46",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID has 1600000 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP has 1598315 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE has 774363 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG has 1 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER has 659775 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT has 1581466 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"TIMESTAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count unique values\n",
    "for column in Columns:\n",
    "    unique_values = df.select(column).distinct()\n",
    "    unique_count = unique_values.count()\n",
    "    \n",
    "    if unique_count > 0:\n",
    "        print(f\"{column} has {unique_count} unique values:\")\n",
    "    else:\n",
    "        print(f\"{column} has no unique value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8bbe08a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID has no duplicate value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP has 1685 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE has 373151 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG has 1 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER has 254498 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:==============================================>       (173 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT has 8434 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:===================================================>  (189 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"TIMESTAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count duplicate values\n",
    "for column in Columns:\n",
    "    count_df = df.groupBy(column).count()\n",
    "    duplicate_values = count_df.filter(col(\"count\") > 1).count()\n",
    "    \n",
    "    if duplicate_values > 0:\n",
    "        print(f\"{column} has {duplicate_values} duplicate values.\")\n",
    "    else:\n",
    "        print(f\"{column} has no duplicate value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2ebf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+--------------------+\n",
      "| ID|                DATE|           USER|                TEXT|\n",
      "+---+--------------------+---------------+--------------------+\n",
      "|  0|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|\n",
      "|  2|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|\n",
      "|  3|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|\n",
      "|  4|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|\n",
      "+---+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the selected columns\n",
    "df = df.drop(\"TIMESTAMP\", \"FLAG\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af53e15e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|summary|                ID|                DATE|                USER|                TEXT|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|  count|           1600000|             1600000|             1600000|             1600000|\n",
      "|   mean|          799999.5|                null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.35968924535|                null|5.162733218454889E10|                null|\n",
      "|    min|                 0|Fri Apr 17 20:30:...|        000catnap000|                 ...|\n",
      "|    max|           1599999|Wed May 27 07:27:...|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ca4568",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|summary|                ID|                DATE|                USER|                TEXT|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|  count|           1600000|             1600000|             1600000|             1600000|\n",
      "|   mean|          799999.5|                null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.35968924535|                null|5.162733218454889E10|                null|\n",
      "|    min|                 0|Fri Apr 17 20:30:...|        000catnap000|                 ...|\n",
      "|    25%|            399999|                null|             32508.0|                null|\n",
      "|    50%|            799999|                null|            130587.0|                null|\n",
      "|    75%|           1200076|                null|           1100101.0|                null|\n",
      "|    max|           1599999|Wed May 27 07:27:...|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a848ea0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|TEXT                                                                                                                                    |count|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|is poorly sick                                                                                                                          |4    |\n",
      "|This little tree is tiiiiired  25's (and dealing with stupid people) tomorrow and then finishing 10s monday! Raiding every nice         |2    |\n",
      "|at home with a cold                                                                                                                     |2    |\n",
      "|Hangover.                                                                                                                               |2    |\n",
      "|cant fall asleep                                                                                                                        |2    |\n",
      "|Confused                                                                                                                                |5    |\n",
      "|work all day                                                                                                                            |9    |\n",
      "|@tommcfly Arwhh, feel better soon (Y) JLC wont be the same tonight though  I am in food tech atm ;) weeeheeyy  have a lovely walk. x x x|2    |\n",
      "|marley and me is so sad                                                                                                                 |2    |\n",
      "|i have a fever                                                                                                                          |4    |\n",
      "|is not okay.                                                                                                                            |2    |\n",
      "|I want korean bbq so baddd but no one can come with me                                                                                  |2    |\n",
      "|lost my ipod                                                                                                                            |2    |\n",
      "|working all day                                                                                                                         |4    |\n",
      "|Exhausted.                                                                                                                              |4    |\n",
      "|missing my girls                                                                                                                        |2    |\n",
      "|soninho                                                                                                                                 |4    |\n",
      "|@dollforlife I don't know  , But i'll find a way !                                                                                      |2    |\n",
      "|@bngr Working m'dear...                                                                                                                 |2    |\n",
      "|I feel a cold coming on                                                                                                                 |2    |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Do a grouping and counting operation to find duplicate values in the \"TEXT\" column\n",
    "count_df = df.groupBy(\"TEXT\").count()\n",
    "\n",
    "# Filter rows containing duplicate values\n",
    "duplicate_values = count_df.filter(col(\"count\") > 1)\n",
    "\n",
    "# If there are duplicate values, show them\n",
    "if duplicate_values.count() > 0:\n",
    "    print(\"Duplicate values:\")\n",
    "    duplicate_values.show(truncate=False)  # Display column values in full length\n",
    "else:\n",
    "    print(\"No duplicate values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f12ca9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has 1600000 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62cf6457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- USER: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988dd02",
   "metadata": {},
   "source": [
    "## TEXT PRE-PROCESSING\n",
    "\n",
    "standard pre-processing techniques:\n",
    "\n",
    "- Lower casing the corpus \n",
    "- Removing the punctuation \n",
    "- Removing the stopwords \n",
    "- Tokenizing the corpus \n",
    "- Stemming and Lemmatization\n",
    "- Word embeddings using CountVectorizer and TF-IDF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd24a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df = df.select(\"TEXT\")\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e62f5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, lower, regexp_replace\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501d664",
   "metadata": {},
   "source": [
    "#### TEXT CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62dd2a45",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Save as UDF\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C1\", clean_text_udf(col(\"text\")))\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc41c18",
   "metadata": {},
   "source": [
    "#### EXPAND CONTRACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "104ba5b0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contractions(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "# Save as UDF\n",
    "expand_contractions_udf = udf(expand_contractions, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C2\", expand_contractions_udf(col(\"TEXT_C1\")))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404a694",
   "metadata": {},
   "source": [
    "#### CLEAN THE PUNCTUATION CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22b970ed",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define punctuation characters\n",
    "punctuation_characters = r'[!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-./:;<=>\\?@[\\\\]\\^_`{|}~]'\n",
    "\n",
    "# Remove punctuation characters\n",
    "text_df = text_df.withColumn(\"TEXT_C3\", regexp_replace(col(\"TEXT_C2\"), punctuation_characters, \"\"))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5ae2c",
   "metadata": {},
   "source": [
    "#### CLEAN THE STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0f7eec",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |TEXT_C4                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |bummer shoulda got david carr third day                                     |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|upset cannot update facebook texting might cry result school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |dived many times ball managed save 50 rest go bounds                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 643, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "# Download \"stopwords\" from nltk dictionary\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Configure the language as english\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Define the udf \n",
    "remove_stopwords_udf = udf(lambda text: \" \".join([word for word in text.split() if word not in stop_words]), StringType())\n",
    "\n",
    "# Use the UDF in order to remove stopwords and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C4\", remove_stopwords_udf(col(\"TEXT_C3\")))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa6d41",
   "metadata": {},
   "source": [
    "#### IMPLEMENT LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66a6c09f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/hduser/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[Stage 13:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |TEXT_C4                                                                     |TEXT_C5                                                                      |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |bummer shoulda got david carr third day                                     |bummer shoulda got david carr third day                                      |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|upset cannot update facebook texting might cry result school today also blah|upset can not update facebook texting might cry result school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |dived many times ball managed save 50 rest go bounds                        |dived many time ball managed save 50 rest go bound                           |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download 'punkt','averaged_perceptron_tagger','wordnet' from nltk dictionary\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Defining the function that implements the Lemmatization operation as a UDF\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        lemma = lemmatizer.lemmatize(w)\n",
    "        lemmatized_sentence.append(lemma)\n",
    "    lemmatized_text = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Define the UDF\n",
    "lemmatize_text_udf = udf(lemmatize_text, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C5\", lemmatize_text_udf(text_df[\"TEXT_C4\"]))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcdde0c",
   "metadata": {},
   "source": [
    "#### IMPLEMENT STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3c676df",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "|TEXT                                                                                                               |TEXT_C1                                                                                                        |TEXT_C2                                                                                                         |TEXT_C3                                                                                                    |TEXT_C4                                                                     |TEXT_C5                                                                      |TEXT_C6                                                                  |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "|@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D|  - a that's a bummer.  you shoulda got david carr of third day to do it. ;d                                   |  - a that is a bummer.  you shoulda got david carr of third day to do it. ;d                                   |   a that is a bummer  you shoulda got david carr of third day to do it d                                  |bummer shoulda got david carr third day                                     |bummer shoulda got david carr third day                                      |bummer shoulda got david carr third day                                  |\n",
      "|is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!    |is upset that he can't update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it... and might cry as a result  school today also. blah!|is upset that he cannot update his facebook by texting it and might cry as a result  school today also blah|upset cannot update facebook texting might cry result school today also blah|upset can not update facebook texting might cry result school today also blah|upset can not updat facebook text might cri result school today also blah|\n",
      "|@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds                          | i dived many times for the ball. managed to save 50%  the rest go out of bounds                               | i dived many times for the ball. managed to save 50%  the rest go out of bounds                                | i dived many times for the ball managed to save 50  the rest go out of bounds                             |dived many times ball managed save 50 rest go bounds                        |dived many time ball managed save 50 rest go bound                           |dive mani time ball manag save 50 rest go bound                          |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Defining the function that finds word roots as UDF \n",
    "def stem_text(text):\n",
    "    snow = SnowballStemmer('english')\n",
    "    stemmed_sentence = []\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        stemmed_sentence.append(snow.stem(w))\n",
    "    stemmed_text = \" \".join(stemmed_sentence)\n",
    "    return stemmed_text\n",
    "\n",
    "# Define the UDF\n",
    "stem_text_udf = udf(stem_text, StringType())\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"TEXT_C6\", stem_text_udf(text_df[\"TEXT_C5\"]))\n",
    "\n",
    "# Show the dataframe\n",
    "text_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04c4b19",
   "metadata": {},
   "source": [
    "#### IMPLEMENT TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8581422c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "|TEXT_C6                                                                  |tokens                                                                                 |\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "|bummer shoulda got david carr third day                                  |[bummer, shoulda, got, david, carr, third, day]                                        |\n",
      "|upset can not updat facebook text might cri result school today also blah|[upset, can, not, updat, facebook, text, might, cri, result, school, today, also, blah]|\n",
      "|dive mani time ball manag save 50 rest go bound                          |[dive, mani, time, ball, manag, save, 50, rest, go, bound]                             |\n",
      "+-------------------------------------------------------------------------+---------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Function that splits text into tokens using NLTK\n",
    "def tokenize_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# Define the UDF\n",
    "tokenize_text_udf = udf(tokenize_text, ArrayType(StringType()))\n",
    "\n",
    "# Use the UDF and Create new column\n",
    "text_df = text_df.withColumn(\"tokens\", tokenize_text_udf(text_df[\"TEXT_C6\"]))\n",
    "\n",
    "# Show the selected dataframe\n",
    "text_df.select(\"TEXT_C6\", \"tokens\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c9c65",
   "metadata": {},
   "source": [
    "# DUUUUUUUUUUUUR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "220ddf3f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e9c632d2fc4dc2a32750d5b90725b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'FloatProgress' object has no attribute 'style'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19018/2485801053.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# BERT tokenizer'ı yükle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# BERT modelini yükle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1938\u001b[0m                 \u001b[0;31m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m                 \u001b[0mfast_tokenizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFULL_TOKENIZER_FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   1941\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m                     \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1429\u001b[0m                     \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1432\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mdisplayed_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"(…){displayed_name[-20:]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m     progress = tqdm(\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/tqdm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mare_progress_bars_disabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"disable\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplayed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolour\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;31m# Print initial bar state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36mcolour\u001b[0;34m(self, bar_color)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcolour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'container'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbar_color\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FloatProgress' object has no attribute 'style'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# BERT tokenizer'ı yükle\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# BERT modelini yükle\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Modeli değerlendirme modunda kullanın\n",
    "model.eval()\n",
    "\n",
    "# UDF olarak duygu analizi yapacak fonksiyonu tanımlayın\n",
    "def analyze_sentiment(tokens):\n",
    "    # Tokenleri birleştirerek metin oluşturun\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    # Metni BERT için girişe dönüştürün\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Modeli kullanarak duygu analizi yapın\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Sonuçları pozitif, negatif ve nötr olarak etiketleyin\n",
    "    sentiment_labels = [\"Negatif\", \"Nötr\", \"Pozitif\"]\n",
    "    predicted_label = sentiment_labels[torch.argmax(logits, dim=1)]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# UDF'yi tanımlayın\n",
    "analyze_sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "# DataFrame'deki \"tokens\" sütununu kullanarak duygu analizi yapın ve sonuçları yeni bir sütuna ekleyin\n",
    "text_df = text_df.withColumn(\"sentiment_result\", analyze_sentiment_udf(text_df[\"tokens\"]))\n",
    "\n",
    "# Sonuçları görüntüleyin\n",
    "text_df.select(\"tokens\", \"sentiment_result\").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c290830",
   "metadata": {},
   "source": [
    "#### IMPLEMENT TOKENIZATION AND SPLIT WORDS TO ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "665a378e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=================================================>    (185 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|    word| count|\n",
      "+--------+------+\n",
      "|      go|166488|\n",
      "|     get|109960|\n",
      "|     day|101499|\n",
      "|    good| 90628|\n",
      "|    work| 85203|\n",
      "|    love| 83655|\n",
      "|    like| 82963|\n",
      "|    want| 73526|\n",
      "|     got| 70043|\n",
      "|   today| 66095|\n",
      "|     can| 65259|\n",
      "|     not| 65157|\n",
      "|    time| 64404|\n",
      "|   thank| 60670|\n",
      "|    know| 58230|\n",
      "|    miss| 56620|\n",
      "|    back| 56401|\n",
      "|     one| 55913|\n",
      "|     lol| 55388|\n",
      "|     see| 50987|\n",
      "|    feel| 50657|\n",
      "|   think| 50627|\n",
      "|  realli| 50057|\n",
      "|    hope| 43743|\n",
      "|   night| 43285|\n",
      "|   watch| 43119|\n",
      "|    need| 42906|\n",
      "|   still| 42852|\n",
      "|   would| 42612|\n",
      "|    make| 42553|\n",
      "|     new| 42001|\n",
      "|    well| 41074|\n",
      "|     amp| 40710|\n",
      "|    home| 39492|\n",
      "|    look| 39094|\n",
      "|    come| 38618|\n",
      "|      oh| 38017|\n",
      "|       2| 37968|\n",
      "|    much| 36633|\n",
      "|    last| 35765|\n",
      "| twitter| 34450|\n",
      "|tomorrow| 34440|\n",
      "|    morn| 34264|\n",
      "|    wish| 33231|\n",
      "|   great| 33168|\n",
      "|    wait| 31988|\n",
      "|     sad| 31542|\n",
      "|   sleep| 31221|\n",
      "|  though| 31015|\n",
      "|    haha| 30350|\n",
      "+--------+------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:====================================================> (196 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Create The Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"TEXT_C6\", outputCol=\"words\")\n",
    "tokenizer_df = tokenizer.transform(text_df)\n",
    "\n",
    "# Separate words into individual lines\n",
    "tokenizer_df = tokenizer_df.select(explode(col(\"words\")).alias(\"word\"))\n",
    "\n",
    "# Show the dataframe\n",
    "tokenizer_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c4ffb",
   "metadata": {},
   "source": [
    "#### COUNT THE TOKENIZER WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3795341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the tokenizer words\n",
    "tokenizer_df_count = tokenizer_df.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Show the dataframe\n",
    "tokenizer_df_count.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cb2f37",
   "metadata": {},
   "source": [
    "#### SENTIMENT LABEL ( POSITIVE - NEGATIVE - NEUTRAL )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00c03903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|sentiment_label| count|\n",
      "+---------------+------+\n",
      "|       positive|582066|\n",
      "|        neutral|782520|\n",
      "|       negative|235414|\n",
      "+---------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 30:============================================>           (60 + 1) / 75]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Sentimental Analysis Function\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity\n",
    "\n",
    "# Sentiment Label Function\n",
    "def label_sentiment(score):\n",
    "    if score > 0:\n",
    "        return 'positive'\n",
    "    elif score < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "\n",
    "sentiment_udf = udf(get_sentiment, FloatType())\n",
    "label_udf = udf(label_sentiment, StringType())\n",
    "\n",
    "text_df = text_df.withColumn('sentiment_score', sentiment_udf(text_df['TEXT_C6']))\n",
    "text_df = text_df.withColumn('sentiment_label', label_udf(text_df['sentiment_score']))\n",
    "\n",
    "# Etiket dağılımını göster\n",
    "text_df.groupBy('sentiment_label').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3cddeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pozitif Yorum Oranı: 36.379125%\n",
      "Nötr Yorum Oranı: 48.9075%\n",
      "Negatif Yorum Oranı: 14.713375000000001%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Toplam yorum sayısı\n",
    "total_count = text_df.count()\n",
    "\n",
    "# Pozitif, Negatif ve Nötr yorum sayıları\n",
    "positive_count = text_df.filter(text_df.sentiment_label == \"positive\").count()\n",
    "neutral_count = text_df.filter(text_df.sentiment_label == \"neutral\").count()\n",
    "negative_count = text_df.filter(text_df.sentiment_label == \"negative\").count()\n",
    "\n",
    "# Başarı oranını hesaplama (bu durumda, etiket dağılımının yüzdesini göstermektedir)\n",
    "positive_rate = (positive_count / total_count) * 100\n",
    "neutral_rate = (neutral_count / total_count) * 100\n",
    "negative_rate = (negative_count / total_count) * 100\n",
    "\n",
    "print(f\"Positive rate: {positive_rate}%\")\n",
    "print(f\"Neutral rate: {neutral_rate}%\")\n",
    "print(f\"Negative rate: {negative_rate}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6fff9",
   "metadata": {},
   "source": [
    "# TIME SERIES ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78250158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|DATE                        |\n",
      "+----------------------------+\n",
      "|Mon Apr 06 22:19:45 PDT 2009|\n",
      "|Mon Apr 06 22:19:49 PDT 2009|\n",
      "|Mon Apr 06 22:19:53 PDT 2009|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|\n",
      "|Mon Apr 06 22:20:00 PDT 2009|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|\n",
      "|Mon Apr 06 22:20:05 PDT 2009|\n",
      "|Mon Apr 06 22:20:09 PDT 2009|\n",
      "|Mon Apr 06 22:20:16 PDT 2009|\n",
      "|Mon Apr 06 22:20:17 PDT 2009|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|\n",
      "|Mon Apr 06 22:20:22 PDT 2009|\n",
      "|Mon Apr 06 22:20:25 PDT 2009|\n",
      "|Mon Apr 06 22:20:31 PDT 2009|\n",
      "|Mon Apr 06 22:20:34 PDT 2009|\n",
      "+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df = df.select(\"DATE\")\n",
    "date_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b99cb69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
