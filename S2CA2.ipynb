{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf5b36e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7070968b",
   "metadata": {},
   "source": [
    "### CONNECTING TO MONGODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39694515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB shell version: 3.2.10\r\n"
     ]
    }
   ],
   "source": [
    "!mongo --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mongosh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc29c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MongoDB\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"DFToMongoDB\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# # #\n",
    "data = [(\"John\", 28), (\"Alice\", 22), (\"Bob\", 32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c80aada",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| John| 28|\n",
      "|Alice| 22|\n",
      "|  Bob| 32|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:===========================================================(1 + 0) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f56ac60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Configure MongoDB Database Connection\n",
    "df.write.format(\"mongodb\") \\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1:27017/\") \\\n",
    "    .option(\"database\",\"sample_db\") \\\n",
    "    .option(\"collection\",\"scb\") \\\n",
    "    .mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e187f1",
   "metadata": {},
   "source": [
    "### CONNECTING TO MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb34813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mysql  Ver 8.0.30-0ubuntu0.22.04.1 for Linux on x86_64 ((Ubuntu))\r\n"
     ]
    }
   ],
   "source": [
    "!mysql --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8edb6b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session for MySQL\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DFToMySQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# # #\n",
    "data = [(\"John\", 28), (\"Alice\", 22), (\"Bob\", 32)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Configure MySQL Database Connection\n",
    "mysql_options = {\n",
    "    \"url\": \"jdbc:mysql://localhost:3306/sample\",  # MySQL bağlantı URL'si\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",  # MySQL JDBC sürücüsü\n",
    "    \"dbtable\": \"yourtable\",  # Hedef MySQL tablo adı\n",
    "    \"user\": \"root\",  # MySQL kullanıcı adı\n",
    "    \"password\": \"password\"  # MySQL parola\n",
    "}\n",
    "\n",
    "# DataFrame'i MySQL veritabanına yükleyin\n",
    "df.write.format(\"jdbc\").options(**mysql_options).mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25733419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hduser/Desktop\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3879a6cd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pwd\r\n",
      "cd Downloads/\r\n",
      "nano zahid.txt\r\n",
      "cat zahid.txt \r\n",
      "-----------------------------------------------\r\n",
      "mysql -u root -p\r\n",
      "Enter password: password\r\n",
      "CREATE DATABASE sample;\r\n",
      "USE sample;\r\n",
      "SHOW TABLES;\r\n",
      "-----------------------------------------------\r\n",
      "Downloads$ ls mon*\r\n",
      "Downloads$ sudo cp mon* /usr/local/spark/jars\r\n",
      "Downloads$ sudo cp ./bson-3.12.12.jar /usr/local/spark/jars\r\n",
      "Downloads$ ls mys*\r\n",
      "Downloads$ sudo cp ./mysql-connector-j-8.0.33.jar /usr/local/spark/jars\r\n",
      "--------------------------------------------------------------------------\r\n",
      "cd /usr/local/spark/jars\r\n",
      "/usr/local/spark/jars$ ls mon*\r\n",
      "/usr/local/spark/jars$ ls mysql*\r\n",
      "--------------------------------------------------------------------------\r\n",
      "mongosh\r\n",
      "/mongodb-linux-x86_64-ubuntu1604-3.2.10$ ./bin/mongod\r\n",
      "--------------------------------------------------------------------------\r\n",
      "https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/3.12.12/\r\n",
      "--------------------------------------------------------------------------\r\n",
      "mysql --version\r\n",
      "mongo --version\r\n"
     ]
    }
   ],
   "source": [
    "!cat zahid.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09369a9a",
   "metadata": {},
   "source": [
    "### READ TO CSV FROM HDFS VIA SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae976e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"HDFSToCSV\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Specify CSV file path throught HDFS\n",
    "hdfs_file_path = \"/ProjectTweets.csv\"\n",
    "\n",
    "# Read CSV file with Spark DataFrame\n",
    "df = spark.read.csv(hdfs_file_path, header=False, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f876af7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|_c0|       _c1|                 _c2|     _c3|            _c4|                 _c5|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show DataFrame First 5 Rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74e9815",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdbe39f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| id| timestamp|                date|    flag|           user|                text|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The first method for renamed the column names\n",
    "df1 = df.withColumnRenamed(\"_c0\", \"id\").withColumnRenamed(\"_c1\", \"timestamp\").withColumnRenamed(\"_c2\", \"date\").withColumnRenamed(\"_c3\", \"flag\").withColumnRenamed(\"_c4\", \"user\").withColumnRenamed(\"_c5\", \"text\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23c856f8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "| ID| TIMESTAMP|                DATE|    FLAG|           USER|                TEXT|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "+---+----------+--------------------+--------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The second method for renamed the column names\n",
    "df = df.selectExpr(\"_c0 as ID\", \"_c1 as TIMESTAMP\", \"_c2 as DATE\", \"_c3 as FLAG\", \"_c4 as USER\", \"_c5 as TEXT\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55ae87a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has 1600000 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ae8b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID has 1600000 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP has 1598315 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE has 774363 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG has 1 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER has 659775 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT has 1581466 unique values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"TIMESTAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count unique values\n",
    "for column in Columns:\n",
    "    unique_values = df.select(column).distinct()\n",
    "    unique_count = unique_values.count()\n",
    "    \n",
    "    if unique_count > 0:\n",
    "        print(f\"{column} has {unique_count} unique values:\")\n",
    "    else:\n",
    "        print(f\"{column} has no unique value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8bbe08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID has no duplicate value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMESTAMP has 1685 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE has 373151 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAG has 1 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER has 254498 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:==============================================>       (173 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT has 8434 duplicate values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:===================================================>  (189 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "columns = [\"ID\", \"TIMESTAMP\", \"DATE\", \"FLAG\", \"USER\", \"TEXT\"]\n",
    "\n",
    "Columns = df.columns\n",
    "\n",
    "# Check out the each column and Count duplicate values\n",
    "for column in Columns:\n",
    "    count_df = df.groupBy(column).count()\n",
    "    duplicate_values = count_df.filter(col(\"count\") > 1).count()\n",
    "    \n",
    "    if duplicate_values > 0:\n",
    "        print(f\"{column} has {duplicate_values} duplicate values.\")\n",
    "    else:\n",
    "        print(f\"{column} has no duplicate value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d2ebf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+--------------------+\n",
      "| ID|                DATE|           USER|                TEXT|\n",
      "+---+--------------------+---------------+--------------------+\n",
      "|  0|Mon Apr 06 22:19:...|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|Mon Apr 06 22:19:...|  scotthamilton|is upset that he ...|\n",
      "|  2|Mon Apr 06 22:19:...|       mattycus|@Kenichan I dived...|\n",
      "|  3|Mon Apr 06 22:19:...|        ElleCTF|my whole body fee...|\n",
      "|  4|Mon Apr 06 22:19:...|         Karoli|@nationwideclass ...|\n",
      "+---+--------------------+---------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop the selected columns\n",
    "df = df.drop(\"TIMESTAMP\", \"FLAG\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af53e15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|summary|                ID|                DATE|                USER|                TEXT|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|  count|           1600000|             1600000|             1600000|             1600000|\n",
      "|   mean|          799999.5|                null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.35968924535|                null|5.162733218454889E10|                null|\n",
      "|    min|                 0|Fri Apr 17 20:30:...|        000catnap000|                 ...|\n",
      "|    max|           1599999|Wed May 27 07:27:...|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ca4568",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|summary|                ID|                DATE|                USER|                TEXT|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "|  count|           1600000|             1600000|             1600000|             1600000|\n",
      "|   mean|          799999.5|                null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.35968924535|                null|5.162733218454889E10|                null|\n",
      "|    min|                 0|Fri Apr 17 20:30:...|        000catnap000|                 ...|\n",
      "|    25%|            399999|                null|             32508.0|                null|\n",
      "|    50%|            799999|                null|            130587.0|                null|\n",
      "|    75%|           1200076|                null|           1100101.0|                null|\n",
      "|    max|           1599999|Wed May 27 07:27:...|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a848ea0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate values:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|TEXT                                                                                                                                    |count|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|is poorly sick                                                                                                                          |4    |\n",
      "|This little tree is tiiiiired  25's (and dealing with stupid people) tomorrow and then finishing 10s monday! Raiding every nice         |2    |\n",
      "|at home with a cold                                                                                                                     |2    |\n",
      "|Hangover.                                                                                                                               |2    |\n",
      "|cant fall asleep                                                                                                                        |2    |\n",
      "|Confused                                                                                                                                |5    |\n",
      "|work all day                                                                                                                            |9    |\n",
      "|@tommcfly Arwhh, feel better soon (Y) JLC wont be the same tonight though  I am in food tech atm ;) weeeheeyy  have a lovely walk. x x x|2    |\n",
      "|marley and me is so sad                                                                                                                 |2    |\n",
      "|i have a fever                                                                                                                          |4    |\n",
      "|is not okay.                                                                                                                            |2    |\n",
      "|I want korean bbq so baddd but no one can come with me                                                                                  |2    |\n",
      "|lost my ipod                                                                                                                            |2    |\n",
      "|working all day                                                                                                                         |4    |\n",
      "|Exhausted.                                                                                                                              |4    |\n",
      "|missing my girls                                                                                                                        |2    |\n",
      "|soninho                                                                                                                                 |4    |\n",
      "|@dollforlife I don't know  , But i'll find a way !                                                                                      |2    |\n",
      "|@bngr Working m'dear...                                                                                                                 |2    |\n",
      "|I feel a cold coming on                                                                                                                 |2    |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Do a grouping and counting operation to find duplicate values in the \"TEXT\" column\n",
    "count_df = df.groupBy(\"TEXT\").count()\n",
    "\n",
    "# Filter rows containing duplicate values\n",
    "duplicate_values = count_df.filter(col(\"count\") > 1)\n",
    "\n",
    "# If there are duplicate values, show them\n",
    "if duplicate_values.count() > 0:\n",
    "    print(\"Duplicate values:\")\n",
    "    duplicate_values.show(truncate=False)  # Display column values in full length\n",
    "else:\n",
    "    print(\"No duplicate values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f12ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has 1600000 rows.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# How many rows does the dataframe \n",
    "row_count = df.count()\n",
    "# Print row_count\n",
    "print(\"DataFrame has {} rows.\".format(row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78250158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|DATE                        |\n",
      "+----------------------------+\n",
      "|Mon Apr 06 22:19:45 PDT 2009|\n",
      "|Mon Apr 06 22:19:49 PDT 2009|\n",
      "|Mon Apr 06 22:19:53 PDT 2009|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|\n",
      "|Mon Apr 06 22:19:57 PDT 2009|\n",
      "|Mon Apr 06 22:20:00 PDT 2009|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|\n",
      "|Mon Apr 06 22:20:03 PDT 2009|\n",
      "|Mon Apr 06 22:20:05 PDT 2009|\n",
      "|Mon Apr 06 22:20:09 PDT 2009|\n",
      "|Mon Apr 06 22:20:16 PDT 2009|\n",
      "|Mon Apr 06 22:20:17 PDT 2009|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|\n",
      "|Mon Apr 06 22:20:19 PDT 2009|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|\n",
      "|Mon Apr 06 22:20:20 PDT 2009|\n",
      "|Mon Apr 06 22:20:22 PDT 2009|\n",
      "|Mon Apr 06 22:20:25 PDT 2009|\n",
      "|Mon Apr 06 22:20:31 PDT 2009|\n",
      "|Mon Apr 06 22:20:34 PDT 2009|\n",
      "+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DATE\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62cf6457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- USER: string (nullable = true)\n",
      " |-- TEXT: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff7cdc9",
   "metadata": {},
   "source": [
    "## TEXT PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4edee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57a7fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df15b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
